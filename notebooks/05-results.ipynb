{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression results\n",
    "\n",
    "Logistic regression was performed on the biowulf cluster. \n",
    "\n",
    "All raw outputs are located on biowulf. Tuned outputs are also located in biowulf. \n",
    "\n",
    "Data is backed up in curium but the Jupyter notebooks only exist in biowulf (currently)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes\n",
    "\n",
    "- 2024-05-17\n",
    "    - Added section for KDD figures\n",
    "- 2024-05-14 to 15\n",
    "    - Added Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Run this to load the standard list of models, datas (segmentations), features, C values, and training types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'bert_base_uncased',\n",
    "    'llama_7b', \n",
    "    'llama_13b', \n",
    "    'llama3_8b', \n",
    "    'llama3_8b_instruct',\n",
    "    'mental_bert', \n",
    "    'mental_longformer',\n",
    "    # 'roberta'\n",
    "]\n",
    "    \n",
    "datas = [\n",
    "    'all', \n",
    "    'pt_noshort',\n",
    "    'turns', \n",
    "]\n",
    "\n",
    "features = [\n",
    "    'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', # 'a8', \n",
    "    'b1', 'b2', 'b3', \n",
    "    'c1', 'c2', 'c3', 'c4', \n",
    "    'd1', 'd2', 'd3', 'd4', 'd5', 'd6', \n",
    "    'e1', 'e2', 'e3', # 'e4', # 'e5', \n",
    "    'g1', 'g2', \n",
    "    'f1', 'f2', 'f3', 'f',\n",
    "    'a', 'b', 'c', 'd', 'e', 'g', \n",
    "    'any'\n",
    "]\n",
    "\n",
    "agg_features = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'any']\n",
    "outcomes = list(set(features) - set(agg_features))\n",
    "outcomes.sort()\n",
    "\n",
    "k = 4\n",
    "C_list = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1., 5., 10., 50., 100., 500., 1000., 5000.]\n",
    "trn_types = ['baseline', 'perm_trn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a basic function to return the most common subset: baseline train type and specific outcomes only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_tuned(df, tt='baseline'):\n",
    "    return df[\n",
    "        (df['trn_type'] == tt) & \n",
    "        df['feature'].isin(outcomes)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper-friendly modifications\n",
    "\n",
    "Below are lists to help with converting labels to paper-friendly formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-line names\n",
    "model_names_1 = [\n",
    "    'BERT', \n",
    "    'Llama 2-7B', \n",
    "    'Llama 2-13B', \n",
    "    'Llama 3-8B',\n",
    "    'Llama 3-8B Instruct',\n",
    "    'MentalBERT', \n",
    "    'MentalLongformer'\n",
    "]\n",
    "# Double-line names\n",
    "model_names_2 = [\n",
    "    'BERT', \n",
    "    'Llama 2\\n7B', \n",
    "    'Llama 2\\n13B', \n",
    "    'Llama 3\\n8B',\n",
    "    'Llama 3\\n8B Instr.',\n",
    "    'Mental\\nBERT', \n",
    "    'Mental\\nLongformer'\n",
    "]\n",
    "# Renaming segmentations\n",
    "segmentations = {\n",
    "    'all': 'Original', \n",
    "    'pt_noshort': 'Monologue', \n",
    "    'turns': 'Turns'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "share_path = Path('/Datasets/impactme/results/20240515')\n",
    "share_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating additional results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "tuned_df = pd.read_csv('results/tuned/roc_auc-kfold.csv')\n",
    "\n",
    "# Concatenating the tuned results with roc_auc-kfold-0515\n",
    "tuned_df = pd.concat([\n",
    "    tuned_df, \n",
    "    pd.read_csv('results/tuned/roc_auc-kfold-0515.csv')\n",
    "])\n",
    "\n",
    "tuned_df.to_csv(share_path / 'roc_auc-kfold.csv', index=False)\n",
    "tuned_df.to_csv('results/tuned/roc_auc-kfold.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline vs. permuted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also save a summary df\n",
    "# Reporting ROC AUC's mean across folds and its SD\n",
    "\n",
    "summary_df = tuned_df.groupby(['model', 'data', 'feature', 'trn_type']).agg({'roc_auc': ['mean', 'std']}).reset_index()\n",
    "summary_df.columns = ['model', 'data', 'feature', 'trn_type', 'roc_auc_mean', 'roc_auc_sd']\n",
    "summary_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary df with only trn_type baseline or perm_trn\n",
    "base_v_perm = tuned_df[tuned_df['trn_type'].isin(['baseline', 'perm_trn'])]\n",
    "# Make table wider using trn_type\n",
    "base_v_perm.pivot_table(index=['model', 'data', 'feature'], columns='trn_type', values='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the summary as a CSV\n",
    "summary_df.to_csv('results/tuned/roc_auc-kfold-summary.csv', index=False)\n",
    "summary_df.to_csv(share_path / 'roc_auc-kfold-summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As bar graphs\n",
    "\n",
    "#### Compiling all data\n",
    "\n",
    "Results can be presented as a simple bar graph. \n",
    "\n",
    "These graphs are faceted by dataset. Models are on the x-axis, with the baseline and permutation test side-by-side (color-coded).\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Less visual clutter\n",
    "- Clear demo of permutation test working\n",
    "\n",
    "Disadvantages: \n",
    "\n",
    "- Loss of information across features\n",
    "- Assumes that feature performance can be aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_df['model'].unique()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tuned = pd.read_csv('results/tuned/roc_auc-kfold-summary.csv')\n",
    "baseline = subset_tuned(tuned, 'baseline')\n",
    "perm_trn = subset_tuned(tuned, 'perm_trn')\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for data, ax in zip(datas, axs):\n",
    "    # Create side-by-side bar plot for baseline and perm_trn compared across models\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    heights = [baseline[baseline['model'] == model]['roc_auc_mean'].mean() for model in models]\n",
    "    errbars = [baseline[baseline['model'] == model]['roc_auc_mean'].std() for model in models]\n",
    "    ax.bar(x - width/2, heights, width, yerr=errbars, label='Baseline')\n",
    "\n",
    "    heights = [perm_trn[perm_trn['model'] == model]['roc_auc_mean'].mean() for model in models]\n",
    "    errbars = [perm_trn[perm_trn['model'] == model]['roc_auc_mean'].std() for model in models]\n",
    "    # Manually add offset\n",
    "    ax.bar(x + width/2, heights, width, yerr=errbars, label='Perm.') \n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names_2)\n",
    "    # Make xtick_labels smaller\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "    ax.set_title(segmentations[data])\n",
    "\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "# Add axis labels\n",
    "fig.text(0.5, 0, 'Model', ha='center', va='center', fontsize=12)\n",
    "fig.text(0, 0.5, 'ROC AUC', ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "# fig.suptitle('Average performance at baseline and with training label permutation', fontsize=14)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only for 'any'\n",
    "\n",
    "Here's the bar chart with only 'any' across its four folds. We'll be taking the SD of the ROC AUCs across the outer folds as the error bars on the chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tuned_raw = pd.read_csv('results/tuned/roc_auc-kfold.csv')\n",
    "\n",
    "tuned_raw = pd.concat([\n",
    "    tuned_raw,\n",
    "    pd.read_csv('results/tuned/roc_auc-kfold-0515.csv')\n",
    "])\n",
    "\n",
    "baseline = tuned_raw[tuned_raw['trn_type'] == 'baseline']\n",
    "perm_trn = tuned_raw[tuned_raw['trn_type'] == 'perm_trn']\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for data, ax in zip(datas, axs):\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    heights = [baseline[(baseline['model'] == model) & (baseline['feature'] == 'any')]['roc_auc'].mean() for model in models]\n",
    "    errbars = [baseline[(baseline['model'] == model) & (baseline['feature'] == 'any')]['roc_auc'].std() for model in models]\n",
    "    ax.bar(x - width/2, heights, width, yerr=errbars, label='Baseline')\n",
    "\n",
    "    heights = [perm_trn[(perm_trn['model'] == model) & (perm_trn['feature'] == 'any')]['roc_auc'].mean() for model in models]\n",
    "    errbars = [np.nanstd(perm_trn[(perm_trn['model'] == model) & (perm_trn['feature'] == 'any')]['roc_auc'].values) for model in models]\n",
    "    ax.bar(x + width/2, heights, width, yerr=errbars, label='Perm.')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names_2)\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "    ax.set_title(segmentations[data])\n",
    "\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "# Tight layout\n",
    "plt.tight_layout()\n",
    "fig.text(0.5, 0, 'Model', ha='center', va='center', fontsize=12)\n",
    "fig.text(0, 0.5, 'ROC AUC', ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "# fig.suptitle('Average performance at baseline and with training label permutation', fontsize=14)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of all classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faceted on segmentations, we can plot the performance of each feature for each model as a jittered scatter plot. Here, we will only look at baseline results.\n",
    "\n",
    "The graph produced in this step is systematically jittered by domain so that they are roughly grouped vertically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "   \n",
    "def jitter_dots(dots):\n",
    "    offsets = dots.get_offsets()\n",
    "    jittered_offsets = offsets\n",
    "    # only jitter in the x-direction\n",
    "    jittered_offsets[:, 0] += np.random.uniform(-0.3, 0.3, offsets.shape[0])\n",
    "    dots.set_offsets(jittered_offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper variables indicate the domain and whether a feature is an aggregate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_grp = tuned.copy()\n",
    "tuned_grp = tuned_grp[~tuned_grp['feature'].str.contains('any')]\n",
    "\n",
    "# Create new column 'domain' that is the first character in 'feature' \n",
    "tuned_grp['domain'] = tuned_grp['feature'].str[0]\n",
    "\n",
    "# Create an indicator column 'aggregate' that is True if 'feature' is one letter\n",
    "tuned_grp['aggregate'] = tuned_grp['feature'].str.len() == 1\n",
    "\n",
    "domains = tuned_grp['domain'].unique()\n",
    "\n",
    "tuned_any_temp = tuned[tuned['feature'].str.contains('any')].copy()\n",
    "tuned_any_temp['domain'] = 'any'\n",
    "tuned_any_temp['aggregate'] = True\n",
    "\n",
    "tuned_grp = pd.concat([tuned_grp, tuned_any_temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we facet by segmentation horizontally, so that performance can be tracked horizontally across the segmentations. \n",
    "\n",
    "Although easier to compare segmentations, this produces a very wide graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(datas), figsize=(20, 4))\n",
    "baseline = tuned_grp[tuned_grp['trn_type'] == 'baseline']\n",
    "\n",
    "np.random.seed(402)\n",
    "\n",
    "colors_list = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'olive', 'cyan']\n",
    "colors_list = colors_list[:len(domains)]\n",
    "domain_colors = {domain: color for domain, color in zip(domains, colors_list)}\n",
    "domain_offsets = {\n",
    "    'a': -0.33,\n",
    "    'b': -0.22,\n",
    "    'c': -0.11,\n",
    "    'd': 0.0,\n",
    "    'e': 0.11,\n",
    "    'f': 0.22,\n",
    "    'g': 0.33,\n",
    "    'any': 0.055,\n",
    "}\n",
    "\n",
    "for data, ax in zip(datas, axs):\n",
    "    # Models on the x-axis, ROC AUC on the y-axis\n",
    "\n",
    "    # Plot non-aggregate features first\n",
    "    for i, row in baseline[(baseline['data'] == data) & (~baseline['aggregate'])].iterrows():\n",
    "        color = domain_colors[row['domain']]\n",
    "        marker = f'${row[\"feature\"][1:]}$'\n",
    "        dots = ax.scatter(row['model'], row['roc_auc_mean'], marker=marker, color=color, alpha=0.5)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']] + np.random.uniform(-0.04, 0.04, 1)[0], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    # Plot aggregate features last\n",
    "    for i, row in baseline[(baseline['data'] == data) & (baseline['aggregate'])].iterrows():\n",
    "        color = 'black'\n",
    "        marker = f'${row[\"feature\"].upper()}$'\n",
    "        if row['feature'] == 'any':\n",
    "            marker = 'X'\n",
    "            color = 'red'\n",
    "        dots = ax.scatter(row['model'], row['roc_auc_mean'], marker=marker, color=color, alpha=0.5)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    ax.set_title(data)\n",
    "    ax.set_ylim(0.45, 1.0)\n",
    "    # Increase xlim to accommodate jitter\n",
    "    ax.set_xlim(-0.6, len(models)-0.4)\n",
    "\n",
    "# Create legend\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=domain) for domain, color in zip(domains, colors_list)\n",
    "]\n",
    "handles += [plt.Line2D([0], [0], marker='X', color='w', markerfacecolor='black', label='aggregate')]\n",
    "handles += [plt.Line2D([0], [0], marker='X', color='w', markerfacecolor='red', label='any')]\n",
    "fig.legend(handles=handles, loc='center right')\n",
    "\n",
    "# y-axis label\n",
    "fig.text(0.1, 0.5, 'ROC AUC', ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to facet vertically, at the cost of comparisons across segmentations. This will make the jitter more displaced and clear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(datas), 1, figsize=(10, 10))\n",
    "baseline = tuned_grp[tuned_grp['trn_type'] == 'baseline']\n",
    "\n",
    "np.random.seed(402)\n",
    "\n",
    "colors_list = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'olive', 'cyan']\n",
    "colors_list = colors_list[:len(domains)]\n",
    "domain_colors = {domain: color for domain, color in zip(domains, colors_list)}\n",
    "domain_offsets = {\n",
    "    'a': -0.33,\n",
    "    'b': -0.22,\n",
    "    'c': -0.11,\n",
    "    'd': 0.0,\n",
    "    'e': 0.11,\n",
    "    'f': 0.22,\n",
    "    'g': 0.33,\n",
    "    'any': 0.055,\n",
    "}\n",
    "\n",
    "for data, ax in zip(datas, axs):\n",
    "    # Models on the x-axis, ROC AUC on the y-axis\n",
    "\n",
    "    # Plot non-aggregate features first\n",
    "    for i, row in baseline[(baseline['data'] == data) & (~baseline['aggregate'])].iterrows():\n",
    "        color = domain_colors[row['domain']]\n",
    "        marker = f'${row[\"feature\"][1:]}$'\n",
    "        dots = ax.scatter(row['model'], row['roc_auc_mean'], marker=marker, color=color, alpha=0.35)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']] + np.random.uniform(-0.04, 0.04, 1)[0], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    # Plot aggregate features last\n",
    "    for i, row in baseline[(baseline['data'] == data) & (baseline['aggregate'])].iterrows():\n",
    "        color = 'black'\n",
    "        marker = f'${row[\"feature\"].upper()}$'\n",
    "        if row['feature'] == 'any':\n",
    "            marker = 'X'\n",
    "            color = 'red'\n",
    "        dots = ax.scatter(row['model'], row['roc_auc_mean'], marker=marker, color=color, alpha=.6)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    ax.set_title(segmentations[data])\n",
    "    ax.set_ylim(0.45, 1.0)\n",
    "    # Increase xlim to accommodate jitter\n",
    "    ax.set_xlim(-0.6, len(models)-0.4)\n",
    "\n",
    "    ax.set_xticklabels(model_names_1)\n",
    "\n",
    "# Create legend\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=domain.upper()) for domain, color in zip(domains, colors_list)\n",
    "]\n",
    "handles += [plt.Line2D([0], [0], marker='X', color='w', markerfacecolor='black', label='Domain')]\n",
    "handles += [plt.Line2D([0], [0], marker='X', color='w', markerfacecolor='red', label='Any')]\n",
    "fig.legend(handles=handles, loc='lower center', ncol=len(domains) + 2)\n",
    "\n",
    "# y-axis label\n",
    "fig.text(0.06, 0.5, 'ROC AUC', ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "fig.text(0.5, 0.06, 'Model', ha='center', va='center', fontsize=12)\n",
    "# plt.tight_layout() # tends to cutoff things with the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the above model, but now with the x-axis order being\n",
    "# Llama, MentalLongformer, MentalBERT, and BERT\n",
    "\n",
    "fig, axs = plt.subplots(len(datas), 1, figsize=(10, 10))\n",
    "baseline = tuned_grp[tuned_grp['trn_type'] == 'baseline']\n",
    "# Replace the model names with the paper-friendly format\n",
    "baseline['model_order'] = baseline['model'].replace({\n",
    "    'bert_base_uncased': 0,\n",
    "    'llama_7b': 3,\n",
    "    'mental_bert': 2,\n",
    "    'mental_longformer': 1\n",
    "})\n",
    "models_ordered = ['Llama 2-7B', 'Mental\\nLongformer', 'Mental\\nBERT', 'BERT'][::-1]\n",
    "\n",
    "np.random.seed(402)\n",
    "\n",
    "colors_list = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'olive', 'cyan']\n",
    "colors_list = colors_list[:len(domains)]\n",
    "domain_colors = {domain: color for domain, color in zip(domains, colors_list)}\n",
    "domain_offsets = {\n",
    "    'a': -0.33,\n",
    "    'b': -0.22,\n",
    "    'c': -0.11,\n",
    "    'd': 0.0,\n",
    "    'e': 0.11,\n",
    "    'f': 0.22,\n",
    "    'g': 0.33,\n",
    "    'any': 0.055,\n",
    "}\n",
    "\n",
    "for data, ax in zip(datas, axs):\n",
    "    # Models on the x-axis, ROC AUC on the y-axis\n",
    "\n",
    "    # Plot non-aggregate features first\n",
    "    for i, row in baseline[(baseline['data'] == data) & (~baseline['aggregate'])].iterrows():\n",
    "        color = domain_colors[row['domain']]\n",
    "        marker = f'${row[\"feature\"][1:]}$'\n",
    "        dots = ax.scatter(row['model_order'], row['roc_auc_mean'], marker=marker, color=color, alpha=0.35)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']] + np.random.uniform(-0.04, 0.04, 1)[0], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    # Plot aggregate features last\n",
    "    for i, row in baseline[(baseline['data'] == data) & (baseline['aggregate'])].iterrows():\n",
    "        color = 'black'\n",
    "        marker = f'${row[\"feature\"].upper()}$'\n",
    "        if row['feature'] == 'any':\n",
    "            marker = 'X'\n",
    "            color = 'red'\n",
    "        dots = ax.scatter(row['model_order'], row['roc_auc_mean'], marker=marker, color=color, alpha=.6)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    ax.set_title(segmentations[data])\n",
    "    ax.set_ylim(0.45, 1.0)\n",
    "    # Increase xlim to accommodate jitter\n",
    "    ax.set_xlim(-0.6, len(models)-0.4)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models_ordered)\n",
    "\n",
    "# Create legend\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=domain.upper()) for domain, color in zip(domains, colors_list)\n",
    "]\n",
    "handles += [plt.Line2D([0], [0], marker='X', color='w', markerfacecolor='black', label='Domain')]\n",
    "handles += [plt.Line2D([0], [0], marker='X', color='w', markerfacecolor='red', label='Any')]\n",
    "fig.legend(handles=handles, loc='lower center', ncol=len(domains) + 2)\n",
    "\n",
    "# y-axis label\n",
    "fig.text(0.06, 0.5, 'ROC AUC', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "fig.text(0.5, 0.05, 'Model', ha='center', va='center', fontsize=14)\n",
    "# plt.tight_layout() # tends to cutoff things with the legend\n",
    "# Increase vertical spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the above model, but now with the x-axis order being\n",
    "# Llama, MentalLongformer, MentalBERT, and BERT\n",
    "\n",
    "fig, axs = plt.subplots(len(datas), 1, figsize=(12, 7))\n",
    "baseline = tuned_grp[tuned_grp['trn_type'] == 'baseline']\n",
    "# Replace the model names with the paper-friendly format\n",
    "baseline['model_order'] = baseline['model'].replace({\n",
    "    'bert_base_uncased': 0,\n",
    "    'llama_7b': 3,\n",
    "    'mental_bert': 2,\n",
    "    'mental_longformer': 1\n",
    "})\n",
    "models_ordered = ['Llama 2-7B', 'Mental\\nLongformer', 'Mental\\nBERT', 'BERT'][::-1]\n",
    "\n",
    "np.random.seed(402)\n",
    "\n",
    "colors_list = ['blue', 'green', 'purple', 'orange', 'brown', 'pink', 'olive', 'cyan']\n",
    "colors_list = colors_list[:len(domains)]\n",
    "domain_colors = {domain: color for domain, color in zip(domains, colors_list)}\n",
    "domain_offsets = {\n",
    "    'a': -0.33,\n",
    "    'b': -0.22,\n",
    "    'c': -0.11,\n",
    "    'd': 0.0,\n",
    "    'e': 0.11,\n",
    "    'f': 0.22,\n",
    "    'g': 0.33,\n",
    "    'any': 0.055,\n",
    "}\n",
    "\n",
    "for data, ax in zip(datas, axs):\n",
    "    # Models on the x-axis, ROC AUC on the y-axis\n",
    "\n",
    "    # Plot non-aggregate features first\n",
    "    for i, row in baseline[(baseline['data'] == data) & (~baseline['aggregate'])].iterrows():\n",
    "        color = domain_colors[row['domain']]\n",
    "        marker = f'${row[\"feature\"][1:]}$'\n",
    "        dots = ax.scatter(row['model_order'], row['roc_auc_mean'], marker=marker, color=color, alpha=0.35)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']] + np.random.uniform(-0.04, 0.04, 1)[0], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    # Plot aggregate features last\n",
    "    for i, row in baseline[(baseline['data'] == data) & (baseline['aggregate'])].iterrows():\n",
    "        color = 'black'\n",
    "        marker = f'${row[\"feature\"].upper()}$'\n",
    "        if row['feature'] == 'any':\n",
    "            marker = 'X'\n",
    "            color = 'red'\n",
    "        dots = ax.scatter(row['model_order'], row['roc_auc_mean'], marker=marker, color=color, alpha=.6)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    ax.set_title(segmentations[data])\n",
    "    ax.set_ylim(0.45, 1.0)\n",
    "    # Increase xlim to accommodate jitter\n",
    "    ax.set_xlim(-0.6, len(models)-0.4)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models_ordered)\n",
    "\n",
    "# Create legend\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=domain.upper()) for domain, color in zip(domains, colors_list)\n",
    "]\n",
    "handles += [plt.Line2D([0], [0], marker='X', color='w', markerfacecolor='black', label='Domain')]\n",
    "handles += [plt.Line2D([0], [0], marker='X', color='w', markerfacecolor='red', label='Any')]\n",
    "fig.legend(handles=handles, loc='lower center', ncol=len(domains) + 2)\n",
    "\n",
    "# y-axis label\n",
    "fig.text(0.06, 0.5, 'ROC AUC', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "fig.text(0.5, 0.05, 'Model', ha='center', va='center', fontsize=14)\n",
    "# plt.tight_layout() # tends to cutoff things with the legend\n",
    "# Increase vertical spacing between subplots\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot different segmentations in 3 separate plots\n",
    "\n",
    "baseline = tuned_grp[tuned_grp['trn_type'] == 'baseline']\n",
    "baseline['model_order'] = baseline['model'].replace({\n",
    "    'bert_base_uncased': 0,\n",
    "    'llama_7b': 3,\n",
    "    'mental_bert': 2,\n",
    "    'mental_longformer': 1\n",
    "})\n",
    "models_ordered = ['Llama 2-7B', 'Mental\\nLongformer', 'Mental\\nBERT', 'BERT'][::-1]\n",
    "\n",
    "np.random.seed(430)\n",
    "\n",
    "for data in datas:\n",
    "    # Create new figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "    for i, row in baseline[(baseline['data'] == data) & (~baseline['aggregate'])].iterrows():\n",
    "        color = domain_colors[row['domain']]\n",
    "        marker = f'${row[\"feature\"][1:]}$'\n",
    "        dots = ax.scatter(row['model_order'], row['roc_auc_mean'], marker=marker, color=color, alpha=0.35)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']] + np.random.uniform(-0.04, 0.04, 1)[0], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    for i, row in baseline[(baseline['data'] == data) & (baseline['aggregate'])].iterrows():\n",
    "        color = 'black'\n",
    "        marker = f'${row[\"feature\"].upper()}$'\n",
    "        if row['feature'] == 'any':\n",
    "            marker = 'X'\n",
    "            color = 'red'\n",
    "        dots = ax.scatter(row['model_order'], row['roc_auc_mean'], marker=marker, color=color, alpha=.6)\n",
    "        offsets = dots.get_offsets()\n",
    "        jittered_offsets = offsets + np.array([domain_offsets[row['domain']], 0])\n",
    "        dots.set_offsets(jittered_offsets)\n",
    "\n",
    "    ax.set_title(segmentations[data])\n",
    "    ax.set_ylim(0.45, 1.0)\n",
    "    ax.set_xlim(-0.6, len(models)-0.4)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "\n",
    "    ax.set_xticklabels(models_ordered)\n",
    "    fig.text(0.06, 0.5, 'ROC AUC', ha='center', va='center', rotation='vertical', fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exchanging Llama 2-7B with Llama 3-8B\n",
    "\n",
    "Same dimensions as previous figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posthocs across models withn segmentations\n",
    "\n",
    "All comparisons are done without aggregates. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friedman test\n",
    "\n",
    "We can use a Friedman test to compare the performance of the different models. Because the different datasets are for different use cases, we will make separate comparisons for each dataset without needing a multiple comparison correction. \n",
    "\n",
    "#### On separate folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "import pandas as pd\n",
    "\n",
    "# Read in the tuning results\n",
    "tuned_df = subset_tuned(pd.read_csv('results/tuned/roc_auc-kfold.csv'))\n",
    "\n",
    "for data in datas:\n",
    "    print(f'Friedman test for {data}')\n",
    "    df = tuned_df[tuned_df['data'] == data].pivot(\n",
    "        index=['feature', 'outer_fold'], columns='model', values='roc_auc'\n",
    "    )[models]\n",
    "    # remove rows with NaN\n",
    "    df = df.dropna()\n",
    "    print(friedmanchisquare(*[df[model] for model in models]), '\\n')\n",
    "\n",
    "models_llama = [\n",
    "    'llama_7b', \n",
    "    'llama_13b', \n",
    "    'llama3_8b', \n",
    "    'llama3_8b_instruct'\n",
    "]\n",
    "\n",
    "for data in datas:\n",
    "    print(f'Friedman test for {data}')\n",
    "    df = tuned_df[tuned_df['data'] == data].pivot(\n",
    "        index=['feature', 'outer_fold'], columns='model', values='roc_auc'\n",
    "    )[models]\n",
    "    # remove rows with NaN\n",
    "    df = df.dropna()\n",
    "    print(friedmanchisquare(*[df[model] for model in models_llama]), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaged across folds\n",
    "\n",
    "We can also run the Friedman on averaged ROC AUCs. At $\\alpha = 0.05$, we also receive significant results for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_df = subset_tuned(pd.read_csv('results/tuned/roc_auc-kfold-summary.csv'))\n",
    "\n",
    "for data in datas:\n",
    "    print(f'Friedman test for {data}')\n",
    "    df = tuned_df[tuned_df['data'] == data].pivot(\n",
    "        index='feature', columns='model', values='roc_auc_mean'\n",
    "    )[models]\n",
    "    # remove rows with NaN\n",
    "    df = df.dropna()\n",
    "    print(friedmanchisquare(*[df[model] for model in models]), '\\n')\n",
    "\n",
    "for data in datas:\n",
    "    print(f'Friedman test for {data}')\n",
    "    df = tuned_df[tuned_df['data'] == data].pivot(\n",
    "        index='feature', columns='model', values='roc_auc_mean'\n",
    "    )[models]\n",
    "    df = df.dropna()\n",
    "    print(friedmanchisquare(*[df[model] for model in models_llama]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nemenyi posthoc\n",
    "\n",
    "#### Averaged across folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "\n",
    "# Read in the tuning results\n",
    "tuned_df = subset_tuned(pd.read_csv('results/tuned/roc_auc-kfold-summary.csv'))\n",
    "tuned_df['roc_auc_mean'] = tuned_df['roc_auc_mean'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print means of ROC AUC for each model and dataset\n",
    "for data in datas:\n",
    "    print(f'Means for {data}')\n",
    "    print(tuned_df[tuned_df['data'] == data].groupby('model').agg({'roc_auc_mean': ['mean', 'std']}), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot tuned_df to have models as columns\n",
    "tuned_alt = tuned_df.pivot(\n",
    "    index=['feature', 'data'], columns='model', values='roc_auc_mean'\n",
    ")\n",
    "tuned_alt = pd.DataFrame(tuned_alt.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Model order:', models)\n",
    "\n",
    "for data in datas:\n",
    "    print(f'Nemenyi post-hoc test for {data}')\n",
    "    df = tuned_alt[tuned_alt['data'] == data]\n",
    "    x = np.array([df[model].values for model in models])\n",
    "    print(posthoc_nemenyi_friedman(x.T), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Critical difference diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "from scikit_posthocs import critical_difference_diagram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(5, 10))\n",
    "\n",
    "# Rename models to be appropriate for the figure\n",
    "tuned_df['model_rename'] = tuned_df['model'].replace({\n",
    "    'bert_base_uncased': 'BERT',\n",
    "    'llama_7b': 'Llama 2-7B',\n",
    "    'llama_13b': 'Llama 2-13B',\n",
    "    'llama3_8b': 'Llama 3-8B',\n",
    "    'llama3_8b': 'Llama 3-8B Instr.',\n",
    "    'mental_bert': 'MentalBERT',\n",
    "    'mental_longformer': 'MentalLongformer'\n",
    "})\n",
    "\n",
    "for data, ax in zip(datas, axs):\n",
    "    df = tuned_df[tuned_df['data'] == data]\n",
    "    cd = posthoc_nemenyi_friedman(\n",
    "        df, \n",
    "        melted=True, \n",
    "        block_col='feature', \n",
    "        group_col='model_rename', \n",
    "        y_col='roc_auc_mean'\n",
    "    )\n",
    "    avg_rank = df.groupby('feature').roc_auc_mean.rank(pct=True).groupby(df.model_rename).mean()\n",
    "    # Set axis limits\n",
    "    ax.set_xlim(0.45, 0.8)\n",
    "    critical_difference_diagram(\n",
    "        avg_rank, \n",
    "        cd, \n",
    "        ax=ax, \n",
    "        elbow_props={'color': 'black'}\n",
    "    )\n",
    "    ax.set_title(segmentations[data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian posthoc\n",
    "\n",
    "Bayesian posthocs are calculated on the mean over the outer $k$-folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(20240416)\n",
    "\n",
    "# Read in the baseline and then take mean\n",
    "tuned = subset_tuned(pd.read_csv('results/tuned/roc_auc-kfold-summary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(models)\n",
    "\n",
    "baycomp = {\n",
    "    data: {\n",
    "        'A>B': np.zeros((n, n)),\n",
    "        'A=B': np.zeros((n, n)),\n",
    "        'A<B': np.zeros((n, n))\n",
    "    } for data in datas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from baycomp import two_on_multiple\n",
    "\n",
    "params = list(itertools.combinations(models, 2))\n",
    "for d in datas:\n",
    "    df = tuned[tuned['data'] == d]\n",
    "\n",
    "    for m1, m2 in params:\n",
    "        x = df[df['model'] == m1]['roc_auc_mean'].values\n",
    "        y = df[df['model'] == m2]['roc_auc_mean'].values\n",
    "\n",
    "        result = two_on_multiple(x, y, rope=0.01)\n",
    "\n",
    "        # print(result)\n",
    "\n",
    "        baycomp[d]['A>B'][models.index(m1), models.index(m2)] = result[0]\n",
    "        baycomp[d]['A=B'][models.index(m1), models.index(m2)] = result[1]\n",
    "        baycomp[d]['A<B'][models.index(m1), models.index(m2)] = result[2]\n",
    "\n",
    "        # Fill in the other side of the diagonal as well\n",
    "        baycomp[d]['A<B'][models.index(m2), models.index(m1)] = result[0]\n",
    "        baycomp[d]['A=B'][models.index(m2), models.index(m1)] = result[1]\n",
    "        baycomp[d]['A>B'][models.index(m2), models.index(m1)] = result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability heatmaps\n",
    "\n",
    "##### Original only\n",
    "\n",
    "Our first plots are the results only from the Original segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(25, 6))\n",
    "\n",
    "ind_ = 0 \n",
    "list_matrices = baycomp['all']\n",
    "# Convert dictionary to list\n",
    "list_matrices = [list_matrices['A>B'], list_matrices['A=B'], list_matrices['A<B']]\n",
    "list_prob = ['P(A > B)', 'P(Model Equivalence)', 'P(A < B)']\n",
    "\n",
    "for ind_ in range(len(list_matrices)):    \n",
    "    corrM = np.round(list_matrices[ind_], 2)\n",
    "    mask = np.zeros_like(corrM, dtype=bool)\n",
    "    mask[np.triu_indices_from(corrM)] = True\n",
    "    \n",
    "    sns.heatmap(corrM, mask=mask, annot = True, fmt='.2f',\n",
    "                xticklabels=model_names_2,\n",
    "                yticklabels=model_names_2,\n",
    "                vmin=0, vmax=1, center=0.5,\n",
    "                ax=axs[ind_], cmap=\"Blues\")\n",
    "    axs[ind_].set_title('{}'.format(list_prob[ind_]), fontsize = 22, c='black')\n",
    "    # Label axes\n",
    "    axs[ind_].set_xlabel('Model B', fontsize = 16, c='black')\n",
    "    axs[ind_].set_ylabel('Model A', fontsize = 16, c='black')\n",
    "    \n",
    "    \n",
    "    ind_ = ind_ + 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All comparisons, triangles\n",
    "\n",
    "If we plot each comparison for each segmentation, we have a $n_{comps} \\times n_{segments}$ set of graphs. Here, $n_{comps} = 3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "segmentations = ['Original', 'Monologue', 'Turns']\n",
    "model_names = [\n",
    "    'Llama 2-7B', \n",
    "    'Mental\\nLongformer',\n",
    "    'MentalBERT', \n",
    "    'BERT'\n",
    "]\n",
    "\n",
    "for i, data in enumerate(datas):\n",
    "    list_matrices = baycomp[data]\n",
    "    list_matrices = [list_matrices['A>B'], list_matrices['A=B'], list_matrices['A<B']]\n",
    "\n",
    "    for ind_ in range(len(list_matrices)):    \n",
    "        corrM = np.round(list_matrices[ind_], 2)\n",
    "        mask = np.zeros_like(corrM, dtype=bool)\n",
    "        mask[np.triu_indices_from(corrM)] = True\n",
    "\n",
    "        sns.heatmap(\n",
    "            corrM, mask=mask, \n",
    "            annot = True, fmt='.2f',\n",
    "            xticklabels=model_names,\n",
    "            yticklabels=model_names,\n",
    "            vmin=0, vmax=1,\n",
    "            cbar=False,\n",
    "            ax=axs[i, ind_], cmap=\"Blues\"\n",
    "        )\n",
    "        \n",
    "        # Remove titles for all but the first row\n",
    "        if i == 0:\n",
    "            axs[i, ind_].set_title(f'{list_prob[ind_]}', fontsize = 14, c='black')\n",
    "\n",
    "        # Remove yticks for all but the first column\n",
    "        if ind_ != 0:\n",
    "            axs[i, ind_].set_yticks([])\n",
    "        else:\n",
    "            # Rotate the yticklabels for the first plot\n",
    "            plt.setp(axs[i, ind_].get_yticklabels(), rotation=0)\n",
    "\n",
    "        # Orient xticklabels horizontally\n",
    "        plt.setp(axs[i, ind_].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add Model A to y-axis, Model B to x-axis\n",
    "fig.text(0.5, -0.01, 'Model B', ha='center', va='center', fontsize=14)\n",
    "fig.text(-0.01, 0.5, 'Model A', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "\n",
    "# Label rows on the right axis as the segmentations\n",
    "for i, data in enumerate(datas):\n",
    "    fig.text(\n",
    "        1.0, 0.15 + 0.33 * i, \n",
    "        segmentations[i], \n",
    "        ha='center', \n",
    "        va='center', \n",
    "        # rotation='vertical', \n",
    "        rotation=270,\n",
    "        fontsize=14\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### P(A > B) square (vertical text) (single colorbar)\n",
    "\n",
    "We can also make modifications to use the full square and combine the graphs of $P(A < B)$ and $P(A > B)$. Essentially, we can just flip and rotate one graph to fit on the other. The $P(A = B)$ can then be inferred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, gridspec_kw={'width_ratios': [1, 1, 1, 0.08]}, figsize=(16, 5))\n",
    "axs[0].get_shared_y_axes().join(axs[1], axs[2])\n",
    "\n",
    "for i, data in enumerate(datas[:-1]):\n",
    "    baycomp_mat = baycomp[data]['A>B']\n",
    "    corrM = np.round(baycomp_mat, 2)\n",
    "    mask = np.zeros_like(corrM, dtype=bool)\n",
    "    # Mask the diagonal\n",
    "    np.fill_diagonal(mask, True)\n",
    "\n",
    "    sns.heatmap(\n",
    "        corrM, mask=mask, \n",
    "        annot = True, fmt='.2f',\n",
    "        xticklabels=model_names, yticklabels=model_names,\n",
    "        vmin=0, vmax=1, cbar=False,\n",
    "        ax=axs[i], \n",
    "        cmap=\"Blues\")\n",
    "    axs[i].set_title(f'{segmentations[data]}', fontsize = 15, c='black')\n",
    "\n",
    "data = 'turns'\n",
    "baycomp_mat = baycomp[data]['A>B']\n",
    "corrM = np.round(baycomp_mat, 2)\n",
    "mask = np.zeros_like(corrM, dtype=bool)\n",
    "np.fill_diagonal(mask, True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corrM, mask=mask, \n",
    "    annot = True, fmt='.2f',\n",
    "    xticklabels=model_names, yticklabels=model_names,\n",
    "    vmin=0, vmax=1, \n",
    "    cbar_ax=axs[3], cbar_kws={'label': 'P(A > B)'},\n",
    "    ax=axs[2], \n",
    "    cmap=\"Blues\")\n",
    "axs[2].set_title(f'{segmentations[data]}', fontsize = 15, c='black')\n",
    "\n",
    "plt.tight_layout(pad=1, w_pad=1, h_pad=3.0)\n",
    "fig.text(0.5, -0.01, 'Model B', ha='center', fontsize = 14, c='black')\n",
    "fig.text(-0.01, 0.5, 'Model A', va='center', rotation='vertical', fontsize = 14, c='black')\n",
    "\n",
    "# fig.text(0.5, 1.0, 'Probability that Model A outperforms Model B', ha='center', fontsize = 17, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (In paper) P(A > B) square, one colorbar (main figure)\n",
    "\n",
    "Below is a variation of the above figure without y-axis labels for the subplots besides the first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the above to only have one colorbar\n",
    "fig, axs = plt.subplots(1, 4, gridspec_kw={'width_ratios': [1, 1, 1, 0.08]}, figsize=(16, 5))\n",
    "axs[0].get_shared_y_axes().join(axs[1], axs[2])\n",
    "\n",
    "for i, data in enumerate(datas[:-1]):\n",
    "    baycomp_mat = baycomp[data]['A>B']\n",
    "    corrM = np.round(baycomp_mat, 2)\n",
    "    mask = np.zeros_like(corrM, dtype=bool)\n",
    "    # Mask the diagonal\n",
    "    np.fill_diagonal(mask, True)\n",
    "\n",
    "    sns.heatmap(\n",
    "        corrM, mask=mask, \n",
    "        annot = True, fmt='.2f',\n",
    "        xticklabels=model_names_2, yticklabels=model_names_2,\n",
    "        vmin=0, vmax=1, cbar=False,\n",
    "        ax=axs[i], \n",
    "        cmap=\"Blues\")\n",
    "    axs[i].set_title(f'{segmentations[data]}', fontsize = 15, c='black')\n",
    "\n",
    "data = 'turns'\n",
    "baycomp_mat = baycomp[data]['A>B']\n",
    "corrM = np.round(baycomp_mat, 2)\n",
    "mask = np.zeros_like(corrM, dtype=bool)\n",
    "np.fill_diagonal(mask, True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corrM, mask=mask, \n",
    "    annot = True, fmt='.2f',\n",
    "    xticklabels=model_names_2, yticklabels=model_names_2,\n",
    "    vmin=0, vmax=1, \n",
    "    cbar_ax=axs[3], cbar_kws={'label': 'P(A > B)'},\n",
    "    ax=axs[2], \n",
    "    cmap=\"Blues\")\n",
    "axs[2].set_title(f'{segmentations[data]}', fontsize = 15, c='black')\n",
    "\n",
    "# Remove yticks for all but the first plot\n",
    "for ax in axs[1:-1]:\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Rotate the yticklabels for the first plot\n",
    "plt.setp(axs[0].get_yticklabels(), rotation=0)\n",
    "\n",
    "# Rotate the xticklabels for all plots\n",
    "for ax in axs:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout(pad=1, w_pad=1, h_pad=3.0)\n",
    "fig.text(0.5, -0.01, 'Model B', ha='center', fontsize = 14, c='black')\n",
    "fig.text(-0.01, 0.5, 'Model A', va='center', rotation='vertical', fontsize = 14, c='black')\n",
    "# fig.text(0.5, 1.0, 'Probability that Model A outperforms Model B', ha='center', fontsize = 17, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can supplement the above with figures showing the probability of model equivalence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, gridspec_kw={'width_ratios': [1, 1, 1, 0.08]}, figsize=(17, 5))\n",
    "axs[0].get_shared_y_axes().join(axs[1], axs[2])\n",
    "\n",
    "for i, data in enumerate(datas[:-1]):\n",
    "    baycomp_mat = baycomp[data]['A=B']\n",
    "    corrM = np.round(baycomp_mat, 2)\n",
    "    mask = np.zeros_like(corrM, dtype=bool)\n",
    "    # Mask upper triangle\n",
    "    mask[np.triu_indices_from(corrM)] = True\n",
    "\n",
    "    sns.heatmap(\n",
    "        corrM, mask=mask, \n",
    "        annot = True, fmt='.2f',\n",
    "        xticklabels=model_names_2, yticklabels=model_names_2,\n",
    "        vmin=0, vmax=1, cbar=False,\n",
    "        ax=axs[i], \n",
    "        cmap=\"Blues\")\n",
    "    axs[i].set_title(f'{segmentations[data]}', fontsize = 15, c='black')\n",
    "\n",
    "data = 'turns'\n",
    "baycomp_mat = baycomp[data]['A=B']\n",
    "corrM = np.round(baycomp_mat, 2)\n",
    "mask = np.zeros_like(corrM, dtype=bool)\n",
    "mask[np.triu_indices_from(corrM)] = True\n",
    "\n",
    "sns.heatmap(\n",
    "    corrM, mask=mask, \n",
    "    annot = True, fmt='.2f',\n",
    "    xticklabels=model_names_2, yticklabels=model_names_2,\n",
    "    vmin=0, vmax=1, \n",
    "    cbar_ax=axs[3], cbar_kws={'label': 'P(A = B)'},\n",
    "    ax=axs[2], \n",
    "    cmap=\"Blues\")\n",
    "axs[2].set_title(f'{segmentations[data]}', fontsize = 15, c='black')\n",
    "\n",
    "# Rotate the xticklabels for all plots\n",
    "for ax in axs:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout(pad=.5, w_pad=.5, h_pad=3.0)\n",
    "fig.text(0.5, -0.02, 'Model B', ha='center', fontsize = 14, c='black')\n",
    "fig.text(-0.01, 0.5, 'Model A', va='center', rotation='vertical', fontsize = 14, c='black')\n",
    "\n",
    "# fig.text(0.5, 1.0, 'Probability that Model A outperforms Model B', ha='center', fontsize = 17, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posthocs across segmentation within models\n",
    "\n",
    "Additionally, we can compare model performance across segmentation within models (4 graphs). (Or just Llama 7B). (Specific outcomes only).\n",
    "\n",
    "### Friedman's test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "baseline = subset_tuned(pd.read_csv('results/tuned/roc_auc-kfold-summary.csv'))\n",
    "\n",
    "for model in models:\n",
    "    print(f'Friedman test for {model}')\n",
    "    df = baseline[baseline['model'] == model].pivot(\n",
    "        index='feature', columns='data', values='roc_auc_mean'\n",
    "    )[datas]\n",
    "    # remove rows with NaN\n",
    "    df = df.dropna()\n",
    "    print(friedmanchisquare(*[df[data] for data in datas]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical difference diagrams (Nemenyi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "from scikit_posthocs import critical_difference_diagram\n",
    "\n",
    "baseline['roc_auc'] = baseline['roc_auc_mean'].astype(float)\n",
    "baseline['data_rename'] = baseline['data'].replace(segmentations)\n",
    "\n",
    "fig, axs = plt.subplots(len(models), 1, figsize=(5, 12))\n",
    "\n",
    "for model, ax in zip(models, axs):\n",
    "    df = baseline[baseline['model'] == model]\n",
    "    cd = posthoc_nemenyi_friedman(\n",
    "        df, \n",
    "        melted=True, \n",
    "        block_col='feature', \n",
    "        group_col='data_rename', \n",
    "        y_col='roc_auc'\n",
    "    )\n",
    "    avg_rank = df.groupby('feature').roc_auc.rank(pct=True).groupby(df.data_rename).mean()\n",
    "    ax.set_xlim(0.3, 0.9)\n",
    "    critical_difference_diagram(\n",
    "        avg_rank, \n",
    "        cd, \n",
    "        ax=ax, \n",
    "        elbow_props={'color': 'black'}\n",
    "    )\n",
    "    ax.set_title(model_names_1[models.index(model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian comparison test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(20240416)\n",
    "\n",
    "# Read in the baseline and then take mean\n",
    "tuned = subset_tuned(pd.read_csv('results/tuned/roc_auc-kfold-summary.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'llama_7b',\n",
    "    'mental_bert', \n",
    "    'mental_longformer',\n",
    "    'bert_base_uncased'\n",
    "]\n",
    "\n",
    "datas = [\n",
    "    'pt_noshort', \n",
    "    'turns', \n",
    "    'all'\n",
    "]\n",
    "\n",
    "n = len(datas)\n",
    "\n",
    "baycomp = {\n",
    "    model: {\n",
    "        'A>B': np.zeros((n, n)),\n",
    "        'A=B': np.zeros((n, n)),\n",
    "        'A<B': np.zeros((n, n))\n",
    "    } for model in models\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from baycomp import two_on_multiple\n",
    "\n",
    "params = list(itertools.combinations(datas, 2))\n",
    "\n",
    "for model in models:\n",
    "    df = tuned[tuned['model'] == model]\n",
    "\n",
    "    for d1, d2 in params:\n",
    "        x = df[df['data'] == d1]['roc_auc_mean'].values\n",
    "        y = df[df['data'] == d2]['roc_auc_mean'].values\n",
    "\n",
    "        result = two_on_multiple(x, y, rope=0.01)\n",
    "\n",
    "        baycomp[model]['A>B'][datas.index(d1), datas.index(d2)] = result[0]\n",
    "        baycomp[model]['A=B'][datas.index(d1), datas.index(d2)] = result[1]\n",
    "        baycomp[model]['A<B'][datas.index(d1), datas.index(d2)] = result[2]\n",
    "\n",
    "        # Fill in the other side of the diagonal as well\n",
    "        baycomp[model]['A<B'][datas.index(d2), datas.index(d1)] = result[0]\n",
    "        baycomp[model]['A=B'][datas.index(d2), datas.index(d1)] = result[1]\n",
    "        baycomp[model]['A>B'][datas.index(d2), datas.index(d1)] = result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 5, gridspec_kw={'width_ratios': [1, 1, 1, 1, 0.08]}, figsize=(20, 5))\n",
    "axs[0].get_shared_y_axes().join(axs[1], axs[2])\n",
    "\n",
    "data_names = ['Monologue', 'Turns', 'Original']\n",
    "\n",
    "for i, model in enumerate(models[:-1]):\n",
    "    baycomp_mat = baycomp[model]['A>B']\n",
    "    corrM = np.round(baycomp_mat, 2)\n",
    "    mask = np.zeros_like(corrM, dtype=bool)\n",
    "    # Mask the diagonal\n",
    "    np.fill_diagonal(mask, True)\n",
    "\n",
    "    sns.heatmap(\n",
    "        corrM, mask=mask, \n",
    "        annot = True, fmt='.2f',\n",
    "        xticklabels=data_names, yticklabels=data_names,\n",
    "        vmin=0, vmax=1, cbar=False,\n",
    "        ax=axs[i], \n",
    "        cmap=\"Blues\")\n",
    "    axs[i].set_title(f'{model_names_1[i]}', fontsize = 15, c='black')\n",
    "\n",
    "\n",
    "model = 'bert_base_uncased'\n",
    "baycomp_mat = baycomp[model]['A>B']\n",
    "corrM = np.round(baycomp_mat, 2)\n",
    "mask = np.zeros_like(corrM, dtype=bool)\n",
    "np.fill_diagonal(mask, True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corrM, mask=mask, \n",
    "    annot = True, fmt='.2f',\n",
    "    xticklabels=data_names, yticklabels=data_names,\n",
    "    vmin=0, vmax=1, \n",
    "    cbar_ax=axs[4], cbar_kws={'label': 'P(A > B)'},\n",
    "    ax=axs[3], \n",
    "    cmap=\"Blues\")\n",
    "axs[3].set_title(f'BERT', fontsize = 15, c='black')\n",
    "\n",
    "plt.tight_layout(pad=1, w_pad=1, h_pad=3.0)\n",
    "fig.text(0.5, -0.01, 'Segmentation B', ha='center', fontsize = 14, c='black')\n",
    "fig.text(-0.01, 0.5, 'Segmentation A', va='center', rotation='vertical', fontsize = 14, c='black')\n",
    "\n",
    "# fig.text(0.5, 1.0, 'Probability that Model A outperforms Model B', ha='center', fontsize = 17, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the above to a 2 x 2 diagram\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "data_names = ['Monologue', 'Turns', 'Original']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    baycomp_mat = baycomp[model]['A>B']\n",
    "    corrM = np.round(baycomp_mat, 2)\n",
    "    mask = np.zeros_like(corrM, dtype=bool)\n",
    "    # Mask the diagonal\n",
    "    np.fill_diagonal(mask, True)\n",
    "\n",
    "    sns.heatmap(\n",
    "        corrM, mask=mask, \n",
    "        annot = True, fmt='.2f',\n",
    "        xticklabels=data_names, yticklabels=data_names,\n",
    "        vmin=0, vmax=1, cbar=False,\n",
    "        ax=axs[i//2, i%2], \n",
    "        cmap=\"Blues\")\n",
    "    axs[i//2, i%2].set_title(f'{model_names_1[i]}', fontsize = 15, c='black')\n",
    "\n",
    "plt.tight_layout(pad=1, w_pad=1, h_pad=3.0)\n",
    "fig.text(0.5, -0.01, 'Segmentation B', ha='center', fontsize = 14, c='black')\n",
    "fig.text(-0.01, 0.5, 'Segmentation A', va='center', rotation='vertical', fontsize = 14, c='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Llama 7B: Nemenyi and Bayesian\n",
    "\n",
    "For the preprint, we can produce the diagrams only for Llama 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, gridspec_kw={'width_ratios': [3, 2]}, figsize=(10, 4))\n",
    "\n",
    "baseline = subset_tuned(pd.read_csv('results/tuned/roc_auc-kfold-summary.csv'))\n",
    "baseline['roc_auc'] = baseline['roc_auc_mean'].astype(float)\n",
    "baseline['data_rename'] = baseline['data'].replace(segmentations)\n",
    "\n",
    "# Plot Nemenyi in first plot\n",
    "df = baseline[baseline['model'] == 'llama_7b']\n",
    "cd = posthoc_nemenyi_friedman(\n",
    "    df, \n",
    "    melted=True, \n",
    "    block_col='feature', \n",
    "    group_col='data_rename', \n",
    "    y_col='roc_auc'\n",
    ")\n",
    "avg_rank = df.groupby('feature').roc_auc.rank(pct=True).groupby(df.data_rename).mean()\n",
    "critical_difference_diagram(\n",
    "    avg_rank, \n",
    "    cd, \n",
    "    ax=axs[0], \n",
    "    elbow_props={'color': 'black'}\n",
    ")\n",
    "axs[0].set_title('Nemenyi posthoc', fontsize=14)\n",
    "\n",
    "# Plot Bayes in second plot\n",
    "baycomp_mat = baycomp['llama_7b']['A>B']\n",
    "corrM = np.round(baycomp_mat, 2)\n",
    "mask = np.zeros_like(corrM, dtype=bool)\n",
    "# Mask the diagonal\n",
    "np.fill_diagonal(mask, True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corrM, mask=mask, \n",
    "    annot = True, fmt='.2f',\n",
    "    xticklabels=data_names, yticklabels=data_names,\n",
    "    vmin=0, vmax=1, cbar=False,\n",
    "    ax=axs[1], \n",
    "    cmap=\"Blues\")\n",
    "axs[1].set_title('Bayesian comparison', fontsize=14)\n",
    "axs[1].set_ylabel('Segmentation A', fontsize=13, c='black')\n",
    "axs[1].set_xlabel('Segmentation B', fontsize=13, c='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation burden\n",
    "\n",
    "A bespoke statistic: the recall (proportion of positives labeled as positive) vs. the proportion of the transcript that needs to be reviewed, given some threshold. \n",
    "\n",
    "This statistic is only reporteed on the 'any' feature, as we would likely be using the model to screen for the presence of any of the specific outcomes in a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.read_csv('results/tuned/concat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_pred = outputs[\n",
    "    (outputs['feature'] == 'any') &\n",
    "    (outputs['trn_type'] == 'baseline')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: aggregated data\n",
    "\n",
    "Like ROC, it is unclear how to best aggregate these curves over folds, and the best approach may be to plot each fold separately. However, when given enough data, such as with the `any` feature, it may be fine to work with the concatenated data to produce the curves. \n",
    "\n",
    "Below is a demonstration of the curves for each fold and the concatenated curve. The model used is BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(13, 4))\n",
    "any_pred_bert = any_pred[any_pred['model'] == 'bert_base_uncased']\n",
    "\n",
    "# USe only any_pred_bert, and plot separate lines for each outer_fold\n",
    "for i, data in enumerate(datas):\n",
    "    for fold in range(4):\n",
    "        df = any_pred_bert[(any_pred_bert['data'] == data) & (any_pred_bert['outer_fold'] == fold)]\n",
    "        y_true = df['y_true'].values\n",
    "        y_prob = df['y_prob'].values\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "        _, recall, thresh = precision_recall_curve(y_true, y_prob)\n",
    "        pos_pred = np.sum(y_prob[:, None] > thresh, axis=0) / len(y_true)\n",
    "        pos_pred = np.insert(pos_pred, 0, 1)\n",
    "        axs[i].plot(pos_pred, recall, label=f'Fold {fold}')\n",
    "    \n",
    "    # Add the curve for all points\n",
    "    df = any_pred_bert[any_pred_bert['data'] == data]\n",
    "    y_true = df['y_true'].values\n",
    "    y_prob = df['y_prob'].values\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    _, recall, thresh = precision_recall_curve(y_true, y_prob)\n",
    "    pos_pred = np.sum(y_prob[:, None] > thresh, axis=0) / len(y_true)\n",
    "    pos_pred = np.insert(pos_pred, 0, 1)\n",
    "    axs[i].plot(pos_pred, recall, label='Concat')  \n",
    "\n",
    "\n",
    "    # Add legend\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(segmentations[data])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison across models within segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "anno_burden = []\n",
    "\n",
    "params = itertools.product(datas, models)\n",
    "for data, model in params:\n",
    "        df = any_pred[\n",
    "            (any_pred['data'] == data) & \n",
    "            (any_pred['model'] == model)\n",
    "        ]\n",
    "        y_true = df['y_true'].values\n",
    "        y_prob = df['y_prob'].values\n",
    "        _, recall, thresh = precision_recall_curve(y_true, y_prob)\n",
    "        pos_pred = np.sum(y_prob[:, None] > thresh, axis=0) / len(y_true)\n",
    "        pos_pred = np.insert(pos_pred, 0, 1)\n",
    "        anno_burden.append(\n",
    "            pd.DataFrame({\n",
    "                'recall': recall,\n",
    "                'pos_pred': pos_pred, \n",
    "                'model': model,\n",
    "                'data': data\n",
    "            })\n",
    "        )\n",
    "\n",
    "anno_burden = pd.concat(anno_burden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model and segmentation, we can also evaluate the proportion of text reviewed that corresponds to 50 percent and 80 percent recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "params = itertools.product(datas, models)\n",
    "\n",
    "target_recalls = [0.5, 0.8, 0.95]\n",
    "anno_burden_intercepts = []\n",
    "\n",
    "for i, data in enumerate(datas):\n",
    "    for model in models:\n",
    "        df = anno_burden[\n",
    "            (anno_burden['data'] == data) & \n",
    "            (anno_burden['model'] == model)\n",
    "        ]\n",
    "        pos_pred = df['pos_pred'].values\n",
    "        recall = df['recall'].values\n",
    "\n",
    "        for target_recall in target_recalls:\n",
    "            anno_burden_intercepts.append(\n",
    "                pd.DataFrame({\n",
    "                    'model': model,\n",
    "                    'data': data,\n",
    "                    'target_recall': [target_recall],\n",
    "                    'pos_pred_need': [pos_pred[np.argmin(recall >= target_recall)]]\n",
    "                })\n",
    "            )\n",
    "\n",
    "anno_burden_intercepts = pd.concat(anno_burden_intercepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "for i, data in enumerate(datas):\n",
    "    for model in models:\n",
    "        df = anno_burden[\n",
    "            (anno_burden['data'] == data) & \n",
    "            (anno_burden['model'] == model)\n",
    "        ]\n",
    "        pos_pred = df['pos_pred'].values\n",
    "        recall = df['recall'].values\n",
    "        axs[i].plot(pos_pred, recall, label=model_names_2[models.index(model)])\n",
    "    \n",
    "    for target_recall in target_recalls:\n",
    "        # Get mean pos_pred_need for all models\n",
    "        pos_pred_need = anno_burden_intercepts[\n",
    "            (anno_burden_intercepts['data'] == data) & \n",
    "            (anno_burden_intercepts['target_recall'] == target_recall)\n",
    "        ]['pos_pred_need'].mean()\n",
    "\n",
    "        if target_recall == 0.95:\n",
    "            print(pos_pred_need)\n",
    "\n",
    "        # Draw horizontal lines for recall values\n",
    "        axs[i].hlines(\n",
    "            target_recall, \n",
    "            -0.1, pos_pred_need, \n",
    "            color='black', linestyle='--', alpha=0.5\n",
    "        )\n",
    "        \n",
    "        # Draw vertical lines at the average of pos_pred_need for all models\n",
    "        axs[i].vlines(\n",
    "            pos_pred_need, \n",
    "            -0.1, target_recall, \n",
    "            color='black', linestyle='--', alpha=0.5\n",
    "        )\n",
    "\n",
    "    axs[i].set_xlim(-0.05, 1.05)\n",
    "    axs[i].set_ylim(-0.05, 1.05)\n",
    "\n",
    "    axs[i].set_title(segmentations[data])\n",
    "    axs[i].legend()\n",
    "\n",
    "\n",
    "fig.text(0.5, 0.0, 'Proportion of text reviewed', ha='center', fontsize=12)\n",
    "fig.text(0.09, 0.5, 'Recall', va='center', rotation='vertical', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_burden_intercepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepts_wide = anno_burden_intercepts.copy()\n",
    "intercepts_wide = intercepts_wide.pivot(\n",
    "    index='model', columns=['data', 'target_recall'], values='pos_pred_need'\n",
    ")\n",
    "# Convert to percentages with 2 decimal points\n",
    "intercepts_wide = intercepts_wide.round(3)\n",
    "# Print to LaTeX\n",
    "print(intercepts_wide.to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables\n",
    "\n",
    "Below are LaTeX tables corresponding to relevant data. \n",
    "\n",
    "### ROC AUC over all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned = pd.read_csv('results/tuned/roc_auc-kfold.csv')\n",
    "del tuned['best_C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new rows corresponding to the averages\n",
    "tuned_avg = tuned.groupby(['data', 'model', 'trn_type', 'feature']).mean().reset_index()\n",
    "tuned_avg['outer_fold'] = 'Mean'\n",
    "# Row bind to tuned\n",
    "tuned = pd.concat([tuned, tuned_avg])\n",
    "tuned.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename models\n",
    "tuned['model'] = tuned['model'].replace({\n",
    "    'bert_base_uncased': 'BERT',\n",
    "    'llama_7b': 'Llama 7B',\n",
    "    'mental_bert': 'MentalBERT',\n",
    "    'mental_longformer': 'MentalLongformer'\n",
    "})\n",
    "\n",
    "# Rename features (to upper)\n",
    "tuned['feature'] = tuned['feature'].str.upper()\n",
    "\n",
    "# Reset the index\n",
    "tuned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All values and folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, doing this in one step throws an IndexError\n",
    "# Merging two separate data frames is fine\n",
    "baseline = tuned[tuned['trn_type'] == 'baseline']\n",
    "baseline_wide = baseline.pivot_table(\n",
    "    index=['data', 'model', 'feature'], columns='outer_fold', values='roc_auc'\n",
    ").to_records()\n",
    "perm_wide = tuned[tuned['trn_type'] == 'perm_trn'].pivot_table(\n",
    "    index=['data', 'model', 'feature'], columns='outer_fold', values='roc_auc'\n",
    ").to_records()\n",
    "\n",
    "# Merge the two wide dataframes\n",
    "wide = pd.merge(\n",
    "    pd.DataFrame(baseline_wide), \n",
    "    pd.DataFrame(perm_wide), \n",
    "    on=['data', 'model', 'feature']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round to 3 decimal places, print to LaTeX\n",
    "wide_sf3 = wide.round(3)\n",
    "print(wide_sf3.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only the averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_avg = pd.read_csv('results/tuned/roc_auc-kfold-summary.csv')\n",
    "tuned_avg = tuned_avg[tuned_avg['trn_type'] == 'baseline']\n",
    "del tuned_avg['trn_type']\n",
    "tuned_avg['model'] = tuned_avg['model'].replace({\n",
    "    'bert_base_uncased': 'BERT',\n",
    "    'llama_7b': 'Llama 7B',\n",
    "    'mental_bert': 'MentalBERT',\n",
    "    'mental_longformer': 'MentalLongformer'\n",
    "})\n",
    "tuned_avg['feature'] = tuned_avg['feature'].str.upper()\n",
    "tuned_avg_wide = tuned_avg.pivot_table(\n",
    "    index=['feature'], \n",
    "    columns=['data', 'model'], \n",
    "    values='roc_auc_mean'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round and print\n",
    "print(tuned_avg_wide.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns of tuned_avg_wide to rankings\n",
    "tuned_rank = tuned_avg_wide.copy()\n",
    "tuned_rank = tuned_rank[~tuned_rank.index.isin(aggs)]\n",
    "tuned_rank = tuned_rank.rank(axis=0, method='min', ascending=False)\n",
    "# Remove rows where feature is in agg_features\n",
    "aggs = [f.upper() for f in agg_features]\n",
    "# Convert to integers\n",
    "tuned_rank = tuned_rank.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column that is the average of the row\n",
    "tuned_rank['avg_rank'] = tuned_rank.mean(axis=1).round(2)\n",
    "# Sort by ascending avg_rank\n",
    "tuned_rank = tuned_rank.sort_values('avg_rank')\n",
    "print(tuned_rank.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impactme",
   "language": "python",
   "name": "impactme"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
