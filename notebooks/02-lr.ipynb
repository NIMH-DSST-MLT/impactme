{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel logistic regression\n",
    "\n",
    "Performed on the NIH Biowulf cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create config files\n",
    "\n",
    "Below are all the parameters to be generated in config files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    'all', \n",
    "    'pt_noshort', \n",
    "    'turns'\n",
    "]\n",
    "# Commented features do not converge due to <3 subjects\n",
    "features = [\n",
    "    'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', # 'a8', \n",
    "    'b1', 'b2', 'b3', \n",
    "    'c1', 'c2', 'c3', 'c4', \n",
    "    'd1', 'd2', 'd3', 'd4', 'd5', 'd6', \n",
    "    'e1', 'e2', 'e3', 'e4', # 'e5', \n",
    "    'f1', 'f2', 'f3',\n",
    "    'g1', 'g2', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', \n",
    "    'any'\n",
    "]\n",
    "\n",
    "seed = 116\n",
    "n_parallel = 60\n",
    "max_iter = 5000 # Use 1000 for testing\n",
    "k_fold = 4\n",
    "\n",
    "models = [\n",
    "    'bert_base_uncased', \n",
    "    'llama_7b',\n",
    "    'llama_13b',\n",
    "    'mental_bert', \n",
    "    'mental_longformer', \n",
    "    # 'roberta'\n",
    "]\n",
    "embed = 'last_avg'\n",
    "\n",
    "C_list = [\n",
    "    0.0001, 0.0005, \n",
    "    0.001, 0.005, \n",
    "    0.01, 0.05, \n",
    "    0.1, 0.5, \n",
    "    1., 5., \n",
    "    10., 50., \n",
    "    100., 500., \n",
    "    1000., 5000.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "config_dir = Path('/data/MLDSST/xinaw/impactme/config/baseline')\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "config_paths = []\n",
    "for data in datas:\n",
    "    for feature in features:\n",
    "        for m in models:\n",
    "            cfg_dat = dict(\n",
    "                data=data,\n",
    "                feature=feature,\n",
    "                seed=seed,\n",
    "                n_parallel=n_parallel,\n",
    "                max_iter=max_iter,\n",
    "                k_fold=k_fold,\n",
    "                model_f=m,\n",
    "                embed=embed,\n",
    "                C_list=C_list,\n",
    "            )\n",
    "            cfg_path = config_dir / f'{m}-{data}-{feature}-config'\n",
    "            cfg_path.write_text(json.dumps(cfg_dat, indent=2))\n",
    "            # Uncomment the below to generate every combination of configs to run\n",
    "            config_paths.append(cfg_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select config paths (optional)\n",
    "\n",
    "Although we could run every config by uncommenting the line in the 'Create config files' section, often we only want to regenerate a small set of the logistic regression fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    'all', \n",
    "    'pt_noshort', \n",
    "    'turns'\n",
    "]\n",
    "\n",
    "# Commented features do not converge due to <3 subjects\n",
    "features = [\n",
    "    'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', # 'a8', \n",
    "    'b1', 'b2', 'b3', \n",
    "    'c1', 'c2', 'c3', 'c4', \n",
    "    'd1', 'd2', 'd3', 'd4', 'd5', 'd6', \n",
    "    'e1', 'e2', 'e3', # 'e4', # 'e5', \n",
    "    'f1', 'f2', 'f3',\n",
    "    'g1', 'g2', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', \n",
    "    'any'\n",
    "]\n",
    "\n",
    "models = [\n",
    "    # 'llama_7b',\n",
    "    'bert_base_uncased', \n",
    "    'mental_bert', \n",
    "    'mental_longformer', \n",
    "    'roberta'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "config_dir = Path('/data/MLDSST/xinaw/impactme/config')\n",
    "config_paths = []\n",
    "\n",
    "params = list(itertools.product(models, datas, features))\n",
    "\n",
    "for model, data, feature in params:\n",
    "    cfg_path = config_dir / f'{model}-{data}-{feature}-config'\n",
    "    config_paths.append(cfg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the fit (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to test\n",
    "# Takes around 1 min\n",
    "! python /data/MLDSST/xinaw/impactme/lr.py {config_paths[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the results look fine\n",
    "import pickle\n",
    "\n",
    "with open('results/raw/bert_base_uncased-all-a1.pkl', 'rb') as f:\n",
    "    res = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the config paths to swarm\n",
    "with open('swarm/baseline.swarm', 'w') as f:\n",
    "    for i in config_paths:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/baseline.py {i}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run swarm\n",
    "\n",
    "This should be a `swarm` command. It is fine to start with a low wall-time and capturing the timeouts later. For example, the first command may be\n",
    "\n",
    "```\n",
    "SBATCH_PARTITION=quick swarm --verbose 4 -g 32 -t 64 -b 6 --time=15 /data/MLDSST/xinaw/impactme/swarm/baseline.swarm\n",
    "```\n",
    "\n",
    "As a rule of thumb, for non Llama models (which range from hidden dimension 512 for Longformer to 768 for BERT), the amount of GB needed is 1/2 the number of threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing files\n",
    "\n",
    "Results can fail to process for a variety of reasons, including\n",
    "\n",
    "- Timeout (or slow convergence)\n",
    "- Exceeding allocated memory or CPUs, causing job termination\n",
    "- Misspecification of a column (this has to be fixed earlier in the pipeline)\n",
    "\n",
    "Although most jobs stay within 6 GB of memory, a few will start to creep into requiring 8 GB. \n",
    "\n",
    "To capture the missing results, often allocating more memory, more threads, and more walltime will fix things. \n",
    "\n",
    "Additionally, failure to fit may occur. This occurs with A8, which often yields combinations of folds with no positive labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing results\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "res_f = os.listdir('results/raw/baseline')\n",
    "res_combos = [f'{i[0]}-{i[1]}-{i[2]}.pkl' for i in itertools.product(models, datas, features)]\n",
    "res_miss = [i.replace('.pkl', '') for i in set(res_combos) - set(res_f)]\n",
    "\n",
    "# Write the config paths to swarm\n",
    "with open('swarm/baseline-2.swarm', 'w') as f:\n",
    "    for i in res_miss:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/baseline.py /data/MLDSST/xinaw/impactme/config/{i}-config\\n')\n",
    "\n",
    "# Count the number of missing files\n",
    "print(f'Missing {len(res_miss)} results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For features that do not converge, manual removal from the `timeout.swarm` file is needed. E5, for example, only has 2 subjects, and is therefore eliminated. \n",
    "\n",
    "For files that continue to timeout, it may be helpful to increase the number of parallel jobs and the thread allotment. Increasing from 16 to 32 often provides enough CPU hours to converge.\n",
    "\n",
    "The number of threads should roughly correspond to the number of parallel jobs. Occasionally, some overlap between batched jobs can occur, leading to the job taking more cores than allocated. Unlike memory overhead, this does not appear to terminate the entire job.\n",
    "\n",
    "```\n",
    "SBATCH_PARTITION=quick swarm --verbose 4 -g 32 -t 32 --time=30 -b 4 /data/MLDSST/xinaw/impactme/swarm/timeout.swarm\n",
    "SBATCH_PARTITION=quick swarm --verbose 4 -g 64 -t 64 --time=60 /data/MLDSST/xinaw/impactme/swarm/timeout.swarm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama instructions \n",
    "\n",
    "*WARNING*: Do not run Llama alongside non-Llama jobs in swarm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'llama3_8b', \n",
    "    'llama3_8b_instruct', \n",
    "    # 'llama_70b'\n",
    "]\n",
    "\n",
    "datas = [\n",
    "    'all', \n",
    "    'pt_noshort', \n",
    "    'turns'\n",
    "]\n",
    "\n",
    "features = [\n",
    "    'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', # 'a8', \n",
    "    'b1', 'b2', 'b3', \n",
    "    'c1', 'c2', 'c3', 'c4', \n",
    "    'd1', 'd2', 'd3', 'd4', 'd5', 'd6', \n",
    "    'e1', 'e2', 'e3', # 'e4', # 'e5', \n",
    "    'f1', 'f2', 'f3',\n",
    "    'g1', 'g2', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', \n",
    "    'any'\n",
    "]\n",
    "\n",
    "llama_path = '/data/MLDSST/xinaw/impactme/config/baseline'\n",
    "llama_path = Path(llama_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase parallel jobs\n",
    "\n",
    "The max number of parallelizations is 256, but at this level, the amount of memory needed (256 GB) exceeds that available on a node on the quick partition. \n",
    "\n",
    "As a rule of thumb, the amount of memory, in GB, needed during swarm is 1:1 with the number of threads and, if possible, it might be safer to use 1.5:1.\n",
    "\n",
    "Additionally, it might be prudent to run 1 or 2 fewer parallel jobs than the number of threads allocated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "llama_path = '/data/MLDSST/xinaw/impactme/config/baseline'\n",
    "Path(llama_path).mkdir(exist_ok=True)\n",
    "llama_path = Path(llama_path)\n",
    "\n",
    "llama_paths = []\n",
    "\n",
    "seed = 116\n",
    "n_parallel = 126\n",
    "max_iter = 5000 # Use 1000 for testing\n",
    "k_fold = 4\n",
    "embed = 'last_avg'\n",
    "\n",
    "C_list = [\n",
    "    0.0001, 0.0005, \n",
    "    0.001, 0.005, \n",
    "    0.01, 0.05, \n",
    "    0.1, 0.5, \n",
    "    1., 5., \n",
    "    10., 50., \n",
    "    100., 500., \n",
    "    1000., 5000.\n",
    "]\n",
    "\n",
    "params = list(itertools.product(models, datas, features))\n",
    "for model, data, feature in params:\n",
    "    cfg_dat = dict(\n",
    "        data=data,\n",
    "        feature=feature,\n",
    "        seed=seed,\n",
    "        n_parallel=n_parallel, \n",
    "        max_iter=max_iter,\n",
    "        k_fold=k_fold,\n",
    "        model_f=model,\n",
    "        embed=embed,\n",
    "        C_list=C_list\n",
    "    )\n",
    "    cfg_path = llama_path / f'{model}-{data}-{feature}-config'\n",
    "    cfg_path.write_text(json.dumps(cfg_dat, indent=2))\n",
    "    llama_paths.append(cfg_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing files\n",
    "\n",
    "Use the same parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "res_f = os.listdir('results/raw/baseline')\n",
    "res_combos = [f'{i[0]}-{i[1]}-{i[2]}.pkl' for i in itertools.product(models, datas, features)]\n",
    "res_miss = [i.replace('.pkl', '') for i in set(res_combos) - set(res_f)]\n",
    "\n",
    "with open('swarm/baseline-llama3.swarm', 'w') as f:\n",
    "    for i in res_miss:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/baseline.py {llama_path}/{i}-config\\n')\n",
    "\n",
    "print(f'Missing {len(res_miss)} Llama files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example call to run Llama in swarm:\n",
    "\n",
    "```\n",
    "SBATCH_PARTITION=quick swarm --verbose 4 -g  128 -t 128 -b 2 --time=120 /data/MLDSST/xinaw/impactme/swarm/baseline-llama3.swarm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Partition training\n",
    "\n",
    "In some cases, it may not be possible for the fit to converge, even with 128 parallel threads. In this case, partitioning the training is suggested. \n",
    "\n",
    "Diving the `C_list` into two allows for use of essentially 256 parallel jobs. \n",
    "\n",
    "Try this as a second option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "from os.path import exists\n",
    "import json\n",
    "\n",
    "llama_path = '/data/MLDSST/xinaw/impactme/config/baseline_parts'\n",
    "Path(llama_path).mkdir(exist_ok=True)\n",
    "llama_path = Path(llama_path)\n",
    "\n",
    "seed = 116\n",
    "n_parallel = 64\n",
    "max_iter = 4000 # Use 1000 for testing\n",
    "k_fold = 4\n",
    "embed = 'last_avg'\n",
    "\n",
    "C_lists = [\n",
    "    [0.0001, 0.0005, 0.001, 0.005],\n",
    "    [0.01, 0.05, 0.1, 0.5], \n",
    "    [1., 5., 10., 50.],\n",
    "    [100., 500., 1000., 5000.]\n",
    "]\n",
    "\n",
    "C_parts = range(len(C_lists))\n",
    "\n",
    "datas = [\n",
    "    'all', \n",
    "    'pt_noshort', \n",
    "    'turns'\n",
    "]\n",
    "\n",
    "features = [\n",
    "    'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', # 'a8', \n",
    "    'b1', 'b2', 'b3', \n",
    "    'c1', 'c2', 'c3', 'c4', \n",
    "    'd1', 'd2', 'd3', 'd4', 'd5', 'd6', \n",
    "    'e1', 'e2', 'e3', # 'e4', # 'e5', \n",
    "    'f1', 'f2', 'f3',\n",
    "    'g1', 'g2', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', \n",
    "    'any'\n",
    "]\n",
    "\n",
    "models = [\n",
    "    'llama3_8b', \n",
    "    'llama3_8b_instruct', \n",
    "    # 'llama_70b'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = Path('results/raw/baseline')\n",
    "res_parts_path = Path('results/raw/baseline_parts')\n",
    "parts_paths = []\n",
    "\n",
    "models = [\n",
    "    'llama3_8b', \n",
    "    'llama3_8b_instruct', \n",
    "    'llama_13b'\n",
    "]\n",
    "\n",
    "params = list(itertools.product(models, datas, features, C_parts))\n",
    "for model, data, feature, C_part in params:\n",
    "\n",
    "    if exists(res_path / f'{model}-{data}-{feature}.pkl'):\n",
    "        continue\n",
    "\n",
    "    if exists(res_parts_path / f'{model}-{data}-{feature}-{C_part}.pkl'):\n",
    "        continue\n",
    "\n",
    "    cfg_dat = dict(\n",
    "        data=data,\n",
    "        feature=feature,\n",
    "        seed=seed,\n",
    "        n_parallel=n_parallel, \n",
    "        max_iter=max_iter,\n",
    "        k_fold=k_fold,\n",
    "        model_f=model,\n",
    "        embed=embed,\n",
    "        C_list=C_lists[C_part],\n",
    "        C_part=C_part\n",
    "    )\n",
    "    cfg_path = llama_path / f'{model}-{data}-{feature}-{C_part}-config'\n",
    "    cfg_path.write_text(json.dumps(cfg_dat, indent=2))\n",
    "    parts_paths.append(cfg_path) \n",
    "\n",
    "print(f'Missing {len(parts_paths)} partitioned files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('swarm/baseline_parts.swarm', 'w') as f:\n",
    "    for i in parts_paths:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/baseline_parts.py {i}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate partitions\n",
    "\n",
    "WARNING: ONLY RUN THIS CODE IF THERE ARE NO MISSING FILES ANYMORE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "res_path = Path('results/raw/baseline')\n",
    "res_parts_path = Path('results/raw/baseline_parts')\n",
    "parts_paths = []\n",
    "\n",
    "params = list(itertools.product(models, datas, features, C_parts))\n",
    "\n",
    "for model, data, feature, C_part in params:\n",
    "    res_dict = {}\n",
    "\n",
    "    if exists(res_path / f'{model}-{data}-{feature}.pkl'):\n",
    "        continue\n",
    "    else: # Use the 0th partition as template\n",
    "        with open(res_parts_path / f'{model}-{data}-{feature}-0.pkl', 'rb') as f:\n",
    "            res_dict = pickle.load(f)\n",
    "    \n",
    "    outputs_list = []\n",
    "    \n",
    "    for C_part in range(len(C_lists)):\n",
    "        with open(res_parts_path / f'{model}-{data}-{feature}-{C_part}.pkl', 'rb') as f:\n",
    "            temp = pickle.load(f)\n",
    "            outputs_list.append(temp['outputs'])\n",
    "\n",
    "    res_dict['outputs'] = pd.concat(outputs_list, ignore_index=True)\n",
    "\n",
    "    # Write the full file\n",
    "    with open(res_path / f'{model}-{data}-{feature}.pkl', 'wb') as f:\n",
    "        pickle.dump(res_dict, f)\n",
    "    \n",
    "    del res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutations tests\n",
    "\n",
    "### Test 1: Permutating all labels at the start\n",
    "\n",
    "When loading the data from the features:\n",
    "\n",
    "1. All labels are permuted\n",
    "2. The outer k fold cross-validation is performed\n",
    "3. The inner k-1 fold cross-validation is performed\n",
    "\n",
    "This type of permutation tests how effective training on uninformative, stable inputs is for the prediction of uninformative, stable outputs. That is, it should illustrate the peak modeling performance on a arbitrary labeling system with the same positive/negative ratio as our original data. \n",
    "\n",
    "We can use the same config files but a different executable. \n",
    "\n",
    "We will delay executing Llama for now, as its compute requirements are slightly different from the BERT models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [\n",
    "    'all', \n",
    "    'pt_noshort', \n",
    "    'turns'\n",
    "]\n",
    "\n",
    "# Commented features do not converge due to <3 subjects\n",
    "features = [\n",
    "    'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', # 'a8', \n",
    "    'b1', 'b2', 'b3', \n",
    "    'c1', 'c2', 'c3', 'c4', \n",
    "    'd1', 'd2', 'd3', 'd4', 'd5', 'd6', \n",
    "    'e1', 'e2', 'e3', # 'e4', # 'e5', \n",
    "    'f1', 'f2', 'f3',\n",
    "    'g1', 'g2', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', \n",
    "    'any'\n",
    "]\n",
    "\n",
    "models = [\n",
    "    # llama_7b,\n",
    "    'bert_base_uncased', \n",
    "    'mental_bert', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "config_dir = Path('/data/MLDSST/xinaw/impactme/config')\n",
    "perm_all_paths = []\n",
    "\n",
    "params = list(itertools.product(models, datas, features))\n",
    "\n",
    "for model, data, feature in params:\n",
    "    cfg_path = config_dir / f'{model}-{data}-{feature}-config'\n",
    "    perm_all_paths.append(cfg_path)\n",
    "\n",
    "# Write the config paths to swarm\n",
    "with open('swarm/perm_all.swarm', 'w') as f:\n",
    "    for i in config_paths:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/perm_all.py {i}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Permutation on training sets\n",
    "\n",
    "1. Labels are read as normal\n",
    "2. During inner cross-validation, the training y are permuted. \n",
    "3. During outer cross-validation, the training y are permuted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "config_dir = Path('/data/MLDSST/xinaw/impactme/config/baseline')\n",
    "perm_trn_paths = []\n",
    "\n",
    "datas = [\n",
    "    'all', \n",
    "    'pt_noshort', \n",
    "    'turns'\n",
    "]\n",
    "\n",
    "# Commented features do not converge due to <3 subjects\n",
    "features = [\n",
    "    'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', # 'a8', \n",
    "    'b1', 'b2', 'b3', \n",
    "    'c1', 'c2', 'c3', 'c4', \n",
    "    'd1', 'd2', 'd3', 'd4', 'd5', 'd6', \n",
    "    'e1', 'e2', 'e3', # 'e4', # 'e5', \n",
    "    'f1', 'f2', 'f3',\n",
    "    'g1', 'g2', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', \n",
    "    'any'\n",
    "]\n",
    "\n",
    "models = [\n",
    "    'bert_base_uncased', \n",
    "    'llama_7b', \n",
    "    'llama_13b',\n",
    "    'mental_bert', \n",
    "    'mental_longformer'\n",
    "]\n",
    "    \n",
    "params = list(itertools.product(models, datas, features))\n",
    "\n",
    "for model, data, feature in params:\n",
    "    cfg_path = config_dir / f'{model}-{data}-{feature}-config'\n",
    "    perm_trn_paths.append(cfg_path)\n",
    "\n",
    "# Write the config paths to swarm\n",
    "with open('swarm/perm_trn-llama.swarm', 'w') as f:\n",
    "    for i in perm_trn_paths:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/perm_trn.py {i}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing files\n",
    "\n",
    "The first portion checks for non-Llama models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing results\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "models = [\n",
    "    'bert_base_uncased', \n",
    "    'mental_bert', \n",
    "    'mental_longformer', \n",
    "]\n",
    "        \n",
    "res_f = os.listdir('results/raw/perm_trn')\n",
    "res_combos = [f'{i[0]}-{i[1]}-{i[2]}.pkl' for i in itertools.product(models, datas, features)]\n",
    "res_miss = [i.replace('.pkl', '') for i in set(res_combos) - set(res_f)]\n",
    "\n",
    "# Write the config paths to swarm\n",
    "with open('swarm/perm_trn.swarm', 'w') as f:\n",
    "    for i in res_miss:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/perm_trn.py /data/MLDSST/xinaw/impactme/config/baseline/{i}-config\\n')\n",
    "\n",
    "# Count the number of missing files\n",
    "print(f'Missing {len(res_miss)} results.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example call to run Llama in swarm:\n",
    "\n",
    "```\n",
    "SBATCH_PARTITION=quick swarm --verbose 4 -g  128 -t 128 -b 4 --time=60 /data/MLDSST/xinaw/impactme/swarm/perm_trn-llama.swarm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Llama files\n",
    "\n",
    "Missing Llama results should be allocated more walltime and run separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing results\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "datas = [\n",
    "    'all', \n",
    "    'pt_noshort', \n",
    "    'turns'\n",
    "]\n",
    "\n",
    "features = [\n",
    "    'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', # 'a8', \n",
    "    'b1', 'b2', 'b3', \n",
    "    'c1', 'c2', 'c3', 'c4', \n",
    "    'd1', 'd2', 'd3', 'd4', 'd5', 'd6', \n",
    "    'e1', 'e2', 'e3', # 'e4', # 'e5', \n",
    "    'f1', 'f2', 'f3',\n",
    "    'g1', 'g2', \n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', \n",
    "    'any'\n",
    "]\n",
    "\n",
    "models = [\n",
    "    'llama3_8b', \n",
    "    'llama3_8b_instruct'\n",
    "]\n",
    "\n",
    "res_f = os.listdir('results/raw/perm_trn')\n",
    "res_combos = [\n",
    "    f'{i[0]}-{i[1]}-{i[2]}.pkl' for i in itertools.product(models, datas, features)\n",
    "]\n",
    "res_miss = [i.replace('.pkl', '') for i in set(res_combos) - set(res_f)]\n",
    "\n",
    "# Write the config paths to swarm\n",
    "with open('swarm/perm_trn_parts-llama3.swarm', 'w') as f:\n",
    "    for i in res_miss:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/perm_trn.py /data/MLDSST/xinaw/impactme/config/baseline/{i}-config\\n')\n",
    "\n",
    "# NOT ALL\n",
    "# Count the number of missing files\n",
    "print(f'Missing {len(res_miss)} results.')\n",
    "print(res_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Optional) Parts\n",
    "\n",
    "Given a couple of permutation runs, it may be necessary to partition the remaining permutations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "from os.path import exists\n",
    "import json\n",
    "\n",
    "llama_path = '/data/MLDSST/xinaw/impactme/config/perm_trn_parts'\n",
    "Path(llama_path).mkdir(exist_ok=True)\n",
    "llama_path = Path(llama_path)\n",
    "\n",
    "seed = 116\n",
    "n_parallel = 64\n",
    "max_iter = 4000 # Use 4000 for parts\n",
    "k_fold = 4\n",
    "embed = 'last_avg'\n",
    "\n",
    "C_lists = [\n",
    "    [0.0001, 0.0005, 0.001, 0.005],\n",
    "    [0.01, 0.05, 0.1, 0.5], \n",
    "    [1., 5., 10., 50.],\n",
    "    [100., 500., 1000., 5000.]\n",
    "]\n",
    "\n",
    "C_parts = range(len(C_lists))\n",
    "\n",
    "models = [\n",
    "    'llama3_8b', \n",
    "    'llama3_8b_instruct'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "res_path = Path('results/raw/perm_trn')\n",
    "res_parts_path = Path('results/raw/perm_trn_parts')\n",
    "parts_paths = []\n",
    "\n",
    "params = list(itertools.product(models, datas, features, C_parts))\n",
    "for model, data, feature, C_part in params:\n",
    "\n",
    "    if exists(res_path / f'{model}-{data}-{feature}.pkl'):\n",
    "        continue\n",
    "\n",
    "    if exists(res_parts_path / f'{model}-{data}-{feature}-{C_part}.pkl'):\n",
    "        continue\n",
    "\n",
    "    cfg_dat = dict(\n",
    "        data=data,\n",
    "        feature=feature,\n",
    "        seed=seed,\n",
    "        n_parallel=n_parallel, \n",
    "        max_iter=max_iter,\n",
    "        k_fold=k_fold,\n",
    "        model_f=model,\n",
    "        embed=embed,\n",
    "        C_list=C_lists[C_part],\n",
    "        C_part=C_part\n",
    "    )\n",
    "    cfg_path = llama_path / f'{model}-{data}-{feature}-{C_part}-config'\n",
    "    cfg_path.write_text(json.dumps(cfg_dat, indent=2))\n",
    "    parts_paths.append(cfg_path) \n",
    "\n",
    "print(f'Missing {len(parts_paths)} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('swarm/perm_trn_parts-llama3.swarm', 'w') as f:\n",
    "    for i in parts_paths:\n",
    "        f.write(f'python /data/MLDSST/xinaw/impactme/perm_trn_parts.py {i}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
