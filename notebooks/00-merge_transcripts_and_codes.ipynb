{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145d03a5-bdba-4d40-8d07-8023c8b5960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from subprocess import run\n",
    "from thefuzz import process\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbdfebc-8eb6-4d87-82f6-03b0fb97df77",
   "metadata": {},
   "source": [
    "# Convert docx files to txt\n",
    "any doc must be converted to docx manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b37a9c-3315-4404-a878-f02512cd45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dir = Path('../data/raw/General')\n",
    "text_dir = Path('../data/raw_text/General')\n",
    "text_dir.mkdir(exist_ok=True, parents=True)\n",
    "output_dir = Path('../data/labeled_text')\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c552cdb5-05ed-4c0d-878a-9ae2166d2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "docdf = pd.DataFrame()\n",
    "docdf['doc_path'] = list(gen_dir.glob('*/*.docx'))\n",
    "docdf.doc_path.apply(lambda x: (text_dir / x.parts[-2]).mkdir(exist_ok=True))\n",
    "docdf['txt_path'] = docdf.doc_path.apply(lambda x: text_dir / x.parts[-2] / x.parts[-1].replace('.docx','.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0055fb1-5073-46c9-9a38-c0fcf1f178d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, row in docdf.iterrows():\n",
    "    if not row.txt_path.exists():\n",
    "        cmd=f'pandoc -i \"{row.doc_path}\" -t plain  >  \"{row.txt_path}\"'\n",
    "        !{cmd}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d21cf72-a183-4c05-bcb3-b4e24d419f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docdf['document'] = docdf.doc_path.apply(lambda x: x.parts[-1])\n",
    "docdf['folder'] = docdf.doc_path.apply(lambda x: x.parts[-2])\n",
    "docdf['subject'] = docdf.document.str.split(' ').str[0]\n",
    "docdf['subject'] = docdf.subject.str.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16fb3a-63f2-4e3c-a987-882c44326c4f",
   "metadata": {},
   "source": [
    "# Make transcript dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a53f61-f55b-4d41-b6ec-b6dc30117e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdfs = []\n",
    "for ix, row in docdf.iterrows():\n",
    "    tmptext = row.txt_path.read_text().split('\\n\\n')\n",
    "\n",
    "    tmpdf = pd.DataFrame()\n",
    "    tmpdf['subject'] = [row.subject] * len(tmptext)\n",
    "    tmpdf['folder'] = [row.folder] * len(tmptext)\n",
    "    tmpdf['document'] = [row.document] * len(tmptext)\n",
    "    tmpdf['txt_path'] = [row.txt_path] * len(tmptext)\n",
    "    tmpdf['line_number'] = range(len(tmptext))\n",
    "    tmpdf['text'] = tmptext\n",
    "    textdfs.append(tmpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3a46bb-8e6d-43e9-8103-c4f46e4014ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = pd.concat(textdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead6e67-3a8f-44e2-bb81-207ce6f0a9f1",
   "metadata": {},
   "source": [
    "# read xlsx and match to transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155a3e77-f5c2-4e40-9f16-5a85ca677d78",
   "metadata": {},
   "source": [
    "## Create a df combining all the excel sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a25acd-a3de-4445-bded-b65728a2d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the names of documents in the excel sheet don't match the files names\n",
    "# this look up table has all of the fixes\n",
    "doc_lut = {\n",
    "    'BEH2311 T2 Therapist.docx': 'BEH2311 Therapist.docx',\n",
    "    'BEH2312 T2 Therapist.docx': 'BEH2312 Therapist.docx',\n",
    "    'BEH2313 T2 Therapist.docx': 'BEH2313 Therapist.docx',\n",
    "    'BEH2314 T2 Therapist.docx': 'BEH2314 Therapist.docx',\n",
    "    'BEH2336 T2 Therapist.docx': 'BEH2336 Therapist.docx',\n",
    "    'BEH2340 T2 Therapist.docx':'BEH2340 Therapist.docx',\n",
    "    'BEH2314 T2 YP.doc': 'BEH2314 T2 YP.docx',\n",
    "    'BEH2315 T2 Therapist (002).docx': 'BEH2315 Therapist.docx',\n",
    "    'BEH2331 T2 T.docx': 'BEH2331 Therapist.docx',\n",
    "    'BEH2332 T2 Therapist JH Transcription.docx': 'BEH2332 Therapist.docx',\n",
    "    'BEH2353 therapist.docx': 'BEH2353 Therapist.docx',\n",
    "    'BEH2360 T2 T.docx': 'BEH2360 Therapist.docx',\n",
    "    'ISL2203 T2 T.docx': 'ISL2203 Therapist.docx',\n",
    "    'ISL2209 Therapist T2.docx': 'ISL2209 Therapist.docx',\n",
    "    'ISL2223 T2 Therapist.docx': 'ISL2223 Therapist.docx',\n",
    "    'ISL2228, T2, YP.docx': 'ISL2228 T2 YP.docx',\n",
    "    'TAV2101 T2 Therapist ES Transcript.docx': 'TAV2101 Therapist.docx',\n",
    "    'TAV2131-T2-YP.docx': 'TAV2131 T2 YP.docx',\n",
    "    'TAV2145 T2 ES': 'TAV2145 T2 ES.docx',\n",
    "    'BEH2352.docx': 'BEH2352 Therapist T2.docx',\n",
    "    'BEH2353 T2 YP SP.docx': 'BEH2353 T2 YP.docx',\n",
    "    'BEH2360 YP T2 JH.docx': 'BEH2360 T2 YP.docx',\n",
    "    'ISL2234 T2 Therapist JH.docx': 'ISL2234 Therapist.docx',\n",
    "    'TAV2117 T2 Parent.doc': 'TAV2117 T2 Parent.docx',\n",
    "    'TAV2134 T2 Therapist.docx': 'TAV2134 Therapist.docx',\n",
    "    'TAV2145 T2 P': 'TAV2145 T2 P.docx',\n",
    "    'TAV2145 T2 YP': 'TAV2145 T2 YP.docx',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8099e7c-db68-4fcf-9a01-051b1f31a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the quotes has a typo in it compared to the document\n",
    "# these values \n",
    "quote_shim = [\n",
    "    ('a1_2:5', lambda x: x.replace(' ow much it ', ' how much it '))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2854066-498b-4ddb-a0a3-7af4edfbf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_dir = Path('../data/raw/symptom_change/')\n",
    "xls = sorted(xl_dir.glob('*.xlsx'))\n",
    "xls = [xx for xx in xls if '~$' not in xx.as_posix()]\n",
    "xldfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca011c8c-3554-4815-ac32-6f65d86ca7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for xl_path in xls:\n",
    "    xl = pd.read_excel(xl_path)\n",
    "    xl['subject'] = xl.Document.str.split().str[0].str.split('.').str[0]\n",
    "    xl['subject'] = xl.subject.str.replace(',','')\n",
    "    xl['subject'] = xl.subject.str.split('-').str[0]\n",
    "\n",
    "    xl['ref_start'] = xl.Reference.str.split(' - ').str[0].astype(int)\n",
    "    xl['ref_end'] = xl.Reference.str.split(' - ').str[-1].astype(int)\n",
    "    xl['filename'] = xl_path.parts[-1].split('.')[0]\n",
    "    xl['code'] = xl_path.parts[-1].split('.')[0].split('_')[0]\n",
    "\n",
    "    xl['document'] = xl.Document.replace(doc_lut)\n",
    "\n",
    "    xldfs.append(xl)\n",
    "xldf = pd.concat(xldfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658b2391-0ff1-47d1-acb7-cf68cbc37955",
   "metadata": {},
   "outputs": [],
   "source": [
    "xldf['uid'] = xldf.code + '_' + xldf.ID\n",
    "# make sure uid is unique\n",
    "assert xldf.groupby('uid').count().max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56957d5b-a4c3-490a-b9ef-dbb61f6999ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid, shim_func in quote_shim:\n",
    "    if (xldf.uid == uid).sum() > 1:\n",
    "        raise ValueError(f'More than one uid matched {uid}')\n",
    "    elif (xldf.uid == uid).sum() == 0:\n",
    "        raise ValueError(f'No uid matched {uid}')\n",
    "    quote_to_fix = xldf.loc[xldf.uid == uid, 'Quotation Content'].values[0]\n",
    "    fixed_quote = shim_func(quote_to_fix)\n",
    "    xldf.loc[xldf.uid == uid, 'Quotation Content'] = fixed_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21494f3c-1eaf-4a36-8463-0721a10d58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for docs that don't match the excel files\n",
    "for idx, df in xldf.groupby('document'):\n",
    "    tmpdf = textdf.query(\"document == @idx\")\n",
    "    if len(tmpdf) == 0:\n",
    "        print(f\"'{idx}': ,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed351d-a4b7-4286-a926-3ca171922cdc",
   "metadata": {},
   "source": [
    "## First pass\n",
    "First pass matching just by picking the utterance with the highest score from thefuzz (uses some tokenization and levenshtein distance). Emperically, scores greater than 86 were good matches. I confirmed that matches were good by finding an exact match for the text from the quote in the matched transcript lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf9a9cd2-2846-4c9e-9434-e92276fdb45d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matchdf = []\n",
    "for idx, df in xldf.groupby('document'):\n",
    "    tmpdf = textdf.query(\"document == @idx\")\n",
    "    for xlix, xlrow in df.iterrows():\n",
    "        quote = xlrow['Quotation Content']\n",
    "        normalized_quote = quote.strip().replace('  ', ' ').replace('  ', ' ')\n",
    "        matched_text, score, match_index = process.extract(quote, tmpdf.text, limit=1)[0]\n",
    "        if score <= 86:\n",
    "            continue\n",
    "        # need to pull in lines corresponding to additional lines separated by \\u2029\n",
    "        if '\\u2029' in quote:\n",
    "            normalized_quote = normalized_quote.replace(\" \\u2029\", \"\\u2029\").replace(\"\\u2029 \", \"\\u2029\")\n",
    "            quote_parts = quote.split('\\u2029')\n",
    "            # find the part of the quote that matched originally\n",
    "            qq_text, qq_score, qq_idx = process.extract(matched_text, pd.Series(quote_parts), limit=1)[0]\n",
    "            \n",
    "            qp_idxs = np.arange(len(quote_parts))\n",
    "            qp_idxs = qp_idxs - qq_idx\n",
    "            match_indices = match_index + qp_idxs\n",
    "            full_match = '\\u2029'.join(tmpdf.loc[match_indices, 'text'].values)\n",
    "            quote_in_match = normalized_quote in full_match.replace('\\n', ' ').replace('^(th)', 'th')\n",
    "\n",
    "        else:\n",
    "            quote_in_match = normalized_quote in matched_text.replace('\\n', ' ').replace('^(th)', 'th')\n",
    "            match_indices = [match_index]\n",
    "            full_match = matched_text\n",
    "        \n",
    "        match_start = match_indices[0]\n",
    "        match_end = match_indices[-1]\n",
    "        for mi in match_indices:\n",
    "            try:\n",
    "                match_row = dict(\n",
    "                    subject=xlrow.subject,\n",
    "                    folder=tmpdf.folder.unique()[0],\n",
    "                    document=xlrow.document,\n",
    "                    line_number=mi,\n",
    "                    uid=xlrow.uid,\n",
    "                    code=xlrow.code,\n",
    "                    ID=xlrow.ID,\n",
    "                    filename=xlrow.filename,\n",
    "                    transcript_text=tmpdf.loc[mi, 'text'],\n",
    "                    full_match=full_match,\n",
    "                    quote=quote,\n",
    "                    quote_in_match=quote_in_match,\n",
    "                    xlrefstart = xlrow.ref_start,\n",
    "                    xlrefend = xlrow.ref_end,\n",
    "                    matchstart = match_start,\n",
    "                    matchend = match_end\n",
    "                )\n",
    "                if mi == match_index:\n",
    "                    match_row['score'] = score\n",
    "                matchdf.append(match_row)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "matchdf = pd.DataFrame(matchdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc843e58-255a-4375-9470-329b0720d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchdf['start_offset'] = (matchdf.xlrefstart - matchdf.matchstart)\n",
    "matchdf['start_offset_doc_std'] = matchdf.groupby('document').start_offset.transform('std')\n",
    "matchdf['end_offset'] = (matchdf.xlrefend - matchdf.matchend)\n",
    "\n",
    "# confirm that start offsets match the end offsets\n",
    "assert len(matchdf.loc[matchdf.start_offset != matchdf.end_offset]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "289c958c-99b2-47dd-90b9-3b55e4276506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_offset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BEH2303 T2 YP.docx</th>\n",
       "      <td>0.455243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISL2221 Therapist.docx</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISL2228 T2 Parent.docx</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        start_offset\n",
       "document                            \n",
       "BEH2303 T2 YP.docx          0.455243\n",
       "ISL2221 Therapist.docx           NaN\n",
       "ISL2228 T2 Parent.docx           NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if offsets are consistent across documents\n",
    "matchdf.groupby('document')[['start_offset']].std().query('start_offset != 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb690a8b-a91a-4a54-b12e-9b0036f88eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='line_number', ylabel='start_offset'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only one document has a shift, and I checked the entries manually and they seem fine.\n",
    "sns.scatterplot(x='line_number', y='start_offset', data=matchdf.query('document == \"BEH2303 T2 YP.docx\"').sort_values(['document', 'line_number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e17b0a6-8379-4ec3-8ead-231ed48722ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that all of the quotes are verbatim in the matching text\n",
    "assert matchdf.quote_in_match.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "791945c8-371e-46ae-a8df-2e334f40fa06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_tmp = xldf.merge(matchdf, how='outer', on=['document', 'uid', 'code', 'ID', 'subject'], indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3725a2cd-e39a-4194-bb42-645af8914410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/6yqczlkd3fb3rjn025dydkym0000gs/T/ipykernel_45084/41485191.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merge_tmp.groupby('_merge').count()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Quotation Content</th>\n",
       "      <th>Reference</th>\n",
       "      <th>subject</th>\n",
       "      <th>ref_start</th>\n",
       "      <th>ref_end</th>\n",
       "      <th>filename_x</th>\n",
       "      <th>code</th>\n",
       "      <th>document</th>\n",
       "      <th>...</th>\n",
       "      <th>quote</th>\n",
       "      <th>quote_in_match</th>\n",
       "      <th>xlrefstart</th>\n",
       "      <th>xlrefend</th>\n",
       "      <th>matchstart</th>\n",
       "      <th>matchend</th>\n",
       "      <th>score</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>start_offset_doc_std</th>\n",
       "      <th>end_offset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_merge</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left_only</th>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_only</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both</th>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>...</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>3045</td>\n",
       "      <td>1042</td>\n",
       "      <td>3045</td>\n",
       "      <td>3043</td>\n",
       "      <td>3045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Document  Quotation Content  Reference  subject  ref_start  \\\n",
       "_merge                                                                         \n",
       "left_only     70        70                 70         70       70         70   \n",
       "right_only     0         0                  0          0        0          0   \n",
       "both        3045      3045               3045       3045     3045       3045   \n",
       "\n",
       "            ref_end  filename_x  code  document  ...  quote  quote_in_match  \\\n",
       "_merge                                           ...                          \n",
       "left_only        70          70    70        70  ...      0               0   \n",
       "right_only        0           0     0         0  ...      0               0   \n",
       "both           3045        3045  3045      3045  ...   3045            3045   \n",
       "\n",
       "            xlrefstart  xlrefend  matchstart  matchend  score  start_offset  \\\n",
       "_merge                                                                        \n",
       "left_only            0         0           0         0      0             0   \n",
       "right_only           0         0           0         0      0             0   \n",
       "both              3045      3045        3045      3045   1042          3045   \n",
       "\n",
       "            start_offset_doc_std  end_offset  \n",
       "_merge                                        \n",
       "left_only                      0           0  \n",
       "right_only                     0           0  \n",
       "both                        3043        3045  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_tmp.groupby('_merge').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cbd25-6af6-4c0d-a21e-6eef46515b7b",
   "metadata": {},
   "source": [
    "This approach matches 3045 entries in the excel files and misses 70. So let's see what we can do about that remainder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e43ffb-4479-4991-8a04-fb3914501884",
   "metadata": {},
   "source": [
    "## Second pass\n",
    "For each quote, chunk the documents into strings of the length of the quote witha rolling window and look for the best levenshtein match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3aba57a2-ab7d-4639-a1ee-9a4bb8c62196",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_missing_matches = merge_tmp.query('_merge == \"left_only\"').document.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92b33706-57e5-40cb-8c38-5ca8bec11e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = xldf.loc[xldf.uid.isin(merge_tmp.query('_merge == \"left_only\"').uid.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28052c76-ea9b-43f2-a6d5-26e8e19f0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_matches = []\n",
    "for midx, xlrow in missing.iterrows():\n",
    "    rowdoc = xlrow.document\n",
    "    tmpdf = textdf.query(\"document == @rowdoc\").copy()\n",
    "    tmpdf['dummy'] = tmpdf.apply(lambda x:[x.line_number for cc in x.text] + [x.line_number], axis=1)\n",
    "    catdoc = '\\u2029'.join(tmpdf.text.values)\n",
    "    doclen = len(catdoc)\n",
    "    catidxs = []\n",
    "    for dd in tmpdf.dummy:\n",
    "        catidxs.extend(dd)\n",
    "    catidxs = catidxs[:-1]\n",
    "    quote = xlrow['Quotation Content']\n",
    "    qlen = len(quote)\n",
    "    normalized_quote = quote.strip().replace('  ', ' ').replace('  ', ' ')\n",
    "    if '\\u2029' in quote:\n",
    "        normalized_quote = normalized_quote.replace(\" \\u2029\", \"\\u2029\").replace(\"\\u2029 \", \"\\u2029\")\n",
    "    \n",
    "    chunks = {}\n",
    "    for start_idx in range(0,(doclen-qlen)):\n",
    "        chunks[start_idx] = catdoc[start_idx:start_idx + qlen]\n",
    "    matched_text, score, match_index = process.extract(normalized_quote, chunks, limit=1)[0]\n",
    "    match_indices = np.unique(catidxs[match_index:match_index+qlen])\n",
    "\n",
    "    match_start = match_indices[0]\n",
    "    match_end = match_indices[-1]\n",
    "    quote_in_match = normalized_quote[3:-3] in matched_text.replace('\\n', ' ').replace('^(th)', 'th').strip('.').strip()\n",
    "    for mi in match_indices:\n",
    "        match_row = dict(\n",
    "            subject=xlrow.subject,\n",
    "            folder=tmpdf.folder.unique()[0],\n",
    "            document=xlrow.document,\n",
    "            line_number=mi,\n",
    "            uid=xlrow.uid,\n",
    "            code=xlrow.code,\n",
    "            ID=xlrow.ID,\n",
    "            filename=xlrow.filename,\n",
    "            transcript_text=tmpdf.loc[mi, 'text'],\n",
    "            full_match=matched_text,\n",
    "            quote=quote,\n",
    "            quote_in_match=quote_in_match,\n",
    "            xlrefstart=xlrow.ref_start,\n",
    "            xlrefend=xlrow.ref_end,\n",
    "            matchstart=match_start,\n",
    "            matchend=match_end,\n",
    "            score=score\n",
    "        )\n",
    "    missing_matches.append(match_row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b61fd302-f7d3-424c-84f1-547022011c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_matches = pd.DataFrame(missing_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12485a86-91f1-4552-a923-a9670f136156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that all the missing matches have a verbatim match\n",
    "assert len(missing_matches.query('not quote_in_match')) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcefef96-f243-480b-a096-8a4c0dd00625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_matches.score.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9dc1693-2a6e-4386-a249-71d765cd53e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchdf['pass'] = 'first'\n",
    "missing_matches['pass'] = 'second'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f17f766-15c2-4ebd-a295-889a8a10f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchdf = pd.concat([matchdf, missing_matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbfc0f97-7490-4363-a107-7f23e850975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tmp = xldf.merge(matchdf, how='outer', on=['document', 'uid', 'code', 'ID', 'subject', 'filename'], indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "758a6e44-a690-47de-b763-5c6d1cecbd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wz/6yqczlkd3fb3rjn025dydkym0000gs/T/ipykernel_45084/41485191.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merge_tmp.groupby('_merge').count()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Document</th>\n",
       "      <th>Quotation Content</th>\n",
       "      <th>Reference</th>\n",
       "      <th>subject</th>\n",
       "      <th>ref_start</th>\n",
       "      <th>ref_end</th>\n",
       "      <th>filename</th>\n",
       "      <th>code</th>\n",
       "      <th>document</th>\n",
       "      <th>...</th>\n",
       "      <th>quote_in_match</th>\n",
       "      <th>xlrefstart</th>\n",
       "      <th>xlrefend</th>\n",
       "      <th>matchstart</th>\n",
       "      <th>matchend</th>\n",
       "      <th>score</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>start_offset_doc_std</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>pass</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_merge</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left_only</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_only</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both</th>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>...</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>3115</td>\n",
       "      <td>1112</td>\n",
       "      <td>3045</td>\n",
       "      <td>3043</td>\n",
       "      <td>3045</td>\n",
       "      <td>3115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  Document  Quotation Content  Reference  subject  ref_start  \\\n",
       "_merge                                                                         \n",
       "left_only      0         0                  0          0        0          0   \n",
       "right_only     0         0                  0          0        0          0   \n",
       "both        3115      3115               3115       3115     3115       3115   \n",
       "\n",
       "            ref_end  filename  code  document  ...  quote_in_match  \\\n",
       "_merge                                         ...                   \n",
       "left_only         0         0     0         0  ...               0   \n",
       "right_only        0         0     0         0  ...               0   \n",
       "both           3115      3115  3115      3115  ...            3115   \n",
       "\n",
       "            xlrefstart  xlrefend  matchstart  matchend  score  start_offset  \\\n",
       "_merge                                                                        \n",
       "left_only            0         0           0         0      0             0   \n",
       "right_only           0         0           0         0      0             0   \n",
       "both              3115      3115        3115      3115   1112          3045   \n",
       "\n",
       "            start_offset_doc_std  end_offset  pass  \n",
       "_merge                                              \n",
       "left_only                      0           0     0  \n",
       "right_only                     0           0     0  \n",
       "both                        3043        3045  3115  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_tmp.groupby('_merge').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28b46e76-b018-4396-810a-c3ee0448a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tmp.to_excel(output_dir / 'merged_labels_and_text.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1e565-999b-41df-ba43-5de9bd1ddbee",
   "metadata": {},
   "source": [
    "All of the excel entries have a transcript match now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dffab-7af8-40b0-9376-1716bf872848",
   "metadata": {},
   "source": [
    "## postprocess matches\n",
    "find the longest utterance that matches a chunk of the quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dce0e1db-2ed7-48fa-a22d-bec5a7a1b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this bert stuff is probably a bit of a nerd snipe\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from scipy.spatial.distance import cosine\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "# chunkdf = []\n",
    "# for uid, df in merge_tmp.loc[merge_tmp.quote.str.contains('\\u2029')].groupby(\"uid\"):\n",
    "\n",
    "#     quote = df.quote.iloc[0]\n",
    "#     quote_tokens = tokenizer(quote, return_tensors='pt', truncation=True)\n",
    "#     quote_ntokens = len(quote_tokens['input_ids'][0])\n",
    "#     quote_embed = model(**quote_tokens)[1].detach().numpy()\n",
    "#     for stepsize in range(1, len(df)):\n",
    "#         for start in range(0, len(df) - stepsize + 1):\n",
    "#             text= '\\n'.join(df.transcript_text.iloc[start:start+stepsize])\n",
    "#             line_tokens = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "#             line_ntokens = len(line_tokens['input_ids'][0])\n",
    "#             line_embed = model(**line_tokens)[1].detach().numpy()\n",
    "#             dist = cosine(quote_embed[0], line_embed[0])\n",
    "#             row = dict(\n",
    "#                 uid=uid,\n",
    "#                 stepsize=stepsize,\n",
    "#                 start=start,\n",
    "#                 end=start+stepsize,\n",
    "#                 text=text,\n",
    "#                 dist=dist,\n",
    "#                 logdis=-np.log10(dist),\n",
    "#                 nlines=len(df),\n",
    "#                 quote_ntokens=quote_ntokens,\n",
    "#                 line_ntokens=line_ntokens\n",
    "#             )\n",
    "#             chunkdf.append(row)\n",
    "# chunkdf = pd.DataFrame(chunkdf)\n",
    "\n",
    "# chunkdf['nlines'] =  chunkdf.groupby(\"uid\").end.transform('max')\n",
    "# chunkdf['weighted_score'] = chunkdf.logdist * (chunkdf.nlines / chunkdf.stepsize)\n",
    "# chunkdf['nlineembeds'] = chunkdf.groupby(\"uid\").stepsize.transform('count')\n",
    "\n",
    "# chunkdf.groupby('uid').nlines.first().reset_index().groupby('nlines').count()\n",
    "\n",
    "# chunkdf.to_excel(output_dir / 'semantic_eval.xlsx')\n",
    "\n",
    "# chunkdf.query('nlines > 2 & nlines < 9').groupby(['uid', 'stepsize']).logdist.max().reset_index().groupby('stepsize').logdist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2047b97-c306-46cc-8d1b-e53062da0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biggest_quote_chunk(quote):\n",
    "    qparts = np.array(quote.split('\\u2029'))\n",
    "    qplen = np.array([len(qp) for qp in qparts])\n",
    "    return qparts[qplen == qplen.max()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19aad9a3-fc18-40df-bf75-01ceeb60b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tmp['biggest_quote_chunk'] = merge_tmp.quote.apply(get_biggest_quote_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43d29b95-c98b-4c31-bb66-368f26ff789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_transcript_match(df):\n",
    "    match, score, line_number = process.extract(df.biggest_quote_chunk.values[0], df.set_index('line_number').transcript_text, limit=1)[0]\n",
    "    return line_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e2e9dce-f6ad-45ef-9248-55fdcadb1d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmatch = pd.DataFrame(merge_tmp.groupby('uid').apply(find_transcript_match, include_groups=False), columns=['transcript_match_to_biggest_quote_chunk']).reset_index()\n",
    "merge_tmp = merge_tmp.merge(tmatch, how='left', on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eeeb6e58-6686-439c-8c89-fe3f716b31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tmp['p_in_transcript'] = merge_tmp.transcript_text.str.lower().str.contains('p:')\n",
    "merge_tmp['i_in_transcript'] = merge_tmp.transcript_text.str.lower().str.contains('i:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d7e216e-abb6-4c2a-a24d-3dcb50de07e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a1</th>\n",
       "      <td>317.0</td>\n",
       "      <td>595.747634</td>\n",
       "      <td>383.631126</td>\n",
       "      <td>41.0</td>\n",
       "      <td>295.00</td>\n",
       "      <td>480.0</td>\n",
       "      <td>855.00</td>\n",
       "      <td>1578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a2</th>\n",
       "      <td>141.0</td>\n",
       "      <td>707.992908</td>\n",
       "      <td>436.640887</td>\n",
       "      <td>93.0</td>\n",
       "      <td>424.00</td>\n",
       "      <td>628.0</td>\n",
       "      <td>876.00</td>\n",
       "      <td>2922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>763.761905</td>\n",
       "      <td>589.573174</td>\n",
       "      <td>155.0</td>\n",
       "      <td>462.00</td>\n",
       "      <td>515.0</td>\n",
       "      <td>676.00</td>\n",
       "      <td>2222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a4</th>\n",
       "      <td>86.0</td>\n",
       "      <td>766.186047</td>\n",
       "      <td>343.301360</td>\n",
       "      <td>109.0</td>\n",
       "      <td>492.00</td>\n",
       "      <td>871.0</td>\n",
       "      <td>1035.00</td>\n",
       "      <td>1589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a5</th>\n",
       "      <td>42.0</td>\n",
       "      <td>661.952381</td>\n",
       "      <td>432.419396</td>\n",
       "      <td>109.0</td>\n",
       "      <td>202.25</td>\n",
       "      <td>732.0</td>\n",
       "      <td>1128.00</td>\n",
       "      <td>1128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a6</th>\n",
       "      <td>36.0</td>\n",
       "      <td>556.388889</td>\n",
       "      <td>342.970243</td>\n",
       "      <td>128.0</td>\n",
       "      <td>218.00</td>\n",
       "      <td>670.5</td>\n",
       "      <td>764.00</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a7</th>\n",
       "      <td>63.0</td>\n",
       "      <td>717.777778</td>\n",
       "      <td>830.623385</td>\n",
       "      <td>239.0</td>\n",
       "      <td>292.00</td>\n",
       "      <td>421.0</td>\n",
       "      <td>644.00</td>\n",
       "      <td>4216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a8</th>\n",
       "      <td>12.0</td>\n",
       "      <td>869.833333</td>\n",
       "      <td>787.185762</td>\n",
       "      <td>172.0</td>\n",
       "      <td>172.00</td>\n",
       "      <td>602.0</td>\n",
       "      <td>1310.00</td>\n",
       "      <td>2222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1</th>\n",
       "      <td>113.0</td>\n",
       "      <td>850.070796</td>\n",
       "      <td>330.737048</td>\n",
       "      <td>212.0</td>\n",
       "      <td>595.00</td>\n",
       "      <td>855.0</td>\n",
       "      <td>1234.00</td>\n",
       "      <td>1342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2</th>\n",
       "      <td>435.0</td>\n",
       "      <td>836.285057</td>\n",
       "      <td>396.096793</td>\n",
       "      <td>177.0</td>\n",
       "      <td>530.00</td>\n",
       "      <td>764.0</td>\n",
       "      <td>1092.00</td>\n",
       "      <td>2119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b3</th>\n",
       "      <td>126.0</td>\n",
       "      <td>936.571429</td>\n",
       "      <td>697.489274</td>\n",
       "      <td>218.0</td>\n",
       "      <td>501.00</td>\n",
       "      <td>790.0</td>\n",
       "      <td>1068.00</td>\n",
       "      <td>4216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1</th>\n",
       "      <td>19.0</td>\n",
       "      <td>699.368421</td>\n",
       "      <td>635.464241</td>\n",
       "      <td>139.0</td>\n",
       "      <td>376.50</td>\n",
       "      <td>471.0</td>\n",
       "      <td>642.00</td>\n",
       "      <td>2208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2</th>\n",
       "      <td>91.0</td>\n",
       "      <td>983.846154</td>\n",
       "      <td>625.929175</td>\n",
       "      <td>67.0</td>\n",
       "      <td>586.00</td>\n",
       "      <td>914.0</td>\n",
       "      <td>1227.50</td>\n",
       "      <td>2911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3</th>\n",
       "      <td>198.0</td>\n",
       "      <td>741.368687</td>\n",
       "      <td>518.100583</td>\n",
       "      <td>126.0</td>\n",
       "      <td>367.25</td>\n",
       "      <td>560.0</td>\n",
       "      <td>1013.00</td>\n",
       "      <td>2911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4</th>\n",
       "      <td>145.0</td>\n",
       "      <td>689.786207</td>\n",
       "      <td>372.276875</td>\n",
       "      <td>118.0</td>\n",
       "      <td>378.00</td>\n",
       "      <td>595.0</td>\n",
       "      <td>904.00</td>\n",
       "      <td>1449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1234.000000</td>\n",
       "      <td>900.900876</td>\n",
       "      <td>223.0</td>\n",
       "      <td>500.00</td>\n",
       "      <td>851.0</td>\n",
       "      <td>2485.00</td>\n",
       "      <td>2485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>84.0</td>\n",
       "      <td>742.369048</td>\n",
       "      <td>482.713859</td>\n",
       "      <td>177.0</td>\n",
       "      <td>389.00</td>\n",
       "      <td>620.5</td>\n",
       "      <td>923.00</td>\n",
       "      <td>2243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>56.0</td>\n",
       "      <td>693.339286</td>\n",
       "      <td>393.934987</td>\n",
       "      <td>139.0</td>\n",
       "      <td>447.25</td>\n",
       "      <td>592.5</td>\n",
       "      <td>890.75</td>\n",
       "      <td>2061.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4</th>\n",
       "      <td>57.0</td>\n",
       "      <td>755.087719</td>\n",
       "      <td>510.906417</td>\n",
       "      <td>136.0</td>\n",
       "      <td>431.00</td>\n",
       "      <td>667.0</td>\n",
       "      <td>812.00</td>\n",
       "      <td>1911.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5</th>\n",
       "      <td>143.0</td>\n",
       "      <td>619.188811</td>\n",
       "      <td>369.099398</td>\n",
       "      <td>112.0</td>\n",
       "      <td>364.00</td>\n",
       "      <td>517.0</td>\n",
       "      <td>756.50</td>\n",
       "      <td>1604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d6</th>\n",
       "      <td>120.0</td>\n",
       "      <td>731.025000</td>\n",
       "      <td>415.541470</td>\n",
       "      <td>127.0</td>\n",
       "      <td>398.00</td>\n",
       "      <td>598.0</td>\n",
       "      <td>943.00</td>\n",
       "      <td>1783.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e1</th>\n",
       "      <td>42.0</td>\n",
       "      <td>753.095238</td>\n",
       "      <td>480.387537</td>\n",
       "      <td>166.0</td>\n",
       "      <td>408.00</td>\n",
       "      <td>619.5</td>\n",
       "      <td>1102.00</td>\n",
       "      <td>1948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e2</th>\n",
       "      <td>276.0</td>\n",
       "      <td>979.753623</td>\n",
       "      <td>720.551933</td>\n",
       "      <td>38.0</td>\n",
       "      <td>512.00</td>\n",
       "      <td>712.0</td>\n",
       "      <td>1196.00</td>\n",
       "      <td>3742.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e3</th>\n",
       "      <td>129.0</td>\n",
       "      <td>1113.565891</td>\n",
       "      <td>980.157304</td>\n",
       "      <td>71.0</td>\n",
       "      <td>425.00</td>\n",
       "      <td>646.0</td>\n",
       "      <td>1359.00</td>\n",
       "      <td>3012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e4</th>\n",
       "      <td>34.0</td>\n",
       "      <td>818.470588</td>\n",
       "      <td>543.664348</td>\n",
       "      <td>185.0</td>\n",
       "      <td>387.25</td>\n",
       "      <td>646.0</td>\n",
       "      <td>1317.50</td>\n",
       "      <td>1729.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e5</th>\n",
       "      <td>9.0</td>\n",
       "      <td>487.888889</td>\n",
       "      <td>227.476617</td>\n",
       "      <td>275.0</td>\n",
       "      <td>275.00</td>\n",
       "      <td>592.0</td>\n",
       "      <td>592.00</td>\n",
       "      <td>923.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>47.0</td>\n",
       "      <td>555.000000</td>\n",
       "      <td>302.050744</td>\n",
       "      <td>110.0</td>\n",
       "      <td>382.50</td>\n",
       "      <td>457.0</td>\n",
       "      <td>739.00</td>\n",
       "      <td>1508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>393.160000</td>\n",
       "      <td>144.560160</td>\n",
       "      <td>35.0</td>\n",
       "      <td>305.00</td>\n",
       "      <td>392.0</td>\n",
       "      <td>480.00</td>\n",
       "      <td>678.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f3</th>\n",
       "      <td>65.0</td>\n",
       "      <td>939.615385</td>\n",
       "      <td>580.593949</td>\n",
       "      <td>124.0</td>\n",
       "      <td>488.00</td>\n",
       "      <td>934.0</td>\n",
       "      <td>1280.00</td>\n",
       "      <td>2292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>633.714286</td>\n",
       "      <td>212.990409</td>\n",
       "      <td>268.0</td>\n",
       "      <td>551.00</td>\n",
       "      <td>617.0</td>\n",
       "      <td>797.00</td>\n",
       "      <td>1101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g2</th>\n",
       "      <td>104.0</td>\n",
       "      <td>948.855769</td>\n",
       "      <td>500.352137</td>\n",
       "      <td>239.0</td>\n",
       "      <td>499.00</td>\n",
       "      <td>861.0</td>\n",
       "      <td>1271.00</td>\n",
       "      <td>2211.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count         mean         std    min     25%    50%      75%     max\n",
       "code                                                                       \n",
       "a1    317.0   595.747634  383.631126   41.0  295.00  480.0   855.00  1578.0\n",
       "a2    141.0   707.992908  436.640887   93.0  424.00  628.0   876.00  2922.0\n",
       "a3     42.0   763.761905  589.573174  155.0  462.00  515.0   676.00  2222.0\n",
       "a4     86.0   766.186047  343.301360  109.0  492.00  871.0  1035.00  1589.0\n",
       "a5     42.0   661.952381  432.419396  109.0  202.25  732.0  1128.00  1128.0\n",
       "a6     36.0   556.388889  342.970243  128.0  218.00  670.5   764.00  1800.0\n",
       "a7     63.0   717.777778  830.623385  239.0  292.00  421.0   644.00  4216.0\n",
       "a8     12.0   869.833333  787.185762  172.0  172.00  602.0  1310.00  2222.0\n",
       "b1    113.0   850.070796  330.737048  212.0  595.00  855.0  1234.00  1342.0\n",
       "b2    435.0   836.285057  396.096793  177.0  530.00  764.0  1092.00  2119.0\n",
       "b3    126.0   936.571429  697.489274  218.0  501.00  790.0  1068.00  4216.0\n",
       "c1     19.0   699.368421  635.464241  139.0  376.50  471.0   642.00  2208.0\n",
       "c2     91.0   983.846154  625.929175   67.0  586.00  914.0  1227.50  2911.0\n",
       "c3    198.0   741.368687  518.100583  126.0  367.25  560.0  1013.00  2911.0\n",
       "c4    145.0   689.786207  372.276875  118.0  378.00  595.0   904.00  1449.0\n",
       "d1     37.0  1234.000000  900.900876  223.0  500.00  851.0  2485.00  2485.0\n",
       "d2     84.0   742.369048  482.713859  177.0  389.00  620.5   923.00  2243.0\n",
       "d3     56.0   693.339286  393.934987  139.0  447.25  592.5   890.75  2061.0\n",
       "d4     57.0   755.087719  510.906417  136.0  431.00  667.0   812.00  1911.0\n",
       "d5    143.0   619.188811  369.099398  112.0  364.00  517.0   756.50  1604.0\n",
       "d6    120.0   731.025000  415.541470  127.0  398.00  598.0   943.00  1783.0\n",
       "e1     42.0   753.095238  480.387537  166.0  408.00  619.5  1102.00  1948.0\n",
       "e2    276.0   979.753623  720.551933   38.0  512.00  712.0  1196.00  3742.0\n",
       "e3    129.0  1113.565891  980.157304   71.0  425.00  646.0  1359.00  3012.0\n",
       "e4     34.0   818.470588  543.664348  185.0  387.25  646.0  1317.50  1729.0\n",
       "e5      9.0   487.888889  227.476617  275.0  275.00  592.0   592.00   923.0\n",
       "f1     47.0   555.000000  302.050744  110.0  382.50  457.0   739.00  1508.0\n",
       "f2     25.0   393.160000  144.560160   35.0  305.00  392.0   480.00   678.0\n",
       "f3     65.0   939.615385  580.593949  124.0  488.00  934.0  1280.00  2292.0\n",
       "g1     21.0   633.714286  212.990409  268.0  551.00  617.0   797.00  1101.0\n",
       "g2    104.0   948.855769  500.352137  239.0  499.00  861.0  1271.00  2211.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_tmp['qlen'] = merge_tmp.quote.apply(lambda x: len(x))\n",
    "merge_tmp.groupby('code').qlen.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21df14-ad9d-4647-a26b-fecf2eb3d989",
   "metadata": {},
   "source": [
    "# Merge results in with transcript text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b62265a5-90db-4752-8ea9-851e9e0d9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_lines = merge_tmp.loc[:, ['uid', 'code', 'subject', 'folder', 'document', 'line_number', 'transcript_match_to_biggest_quote_chunk', 'transcript_text']]\n",
    "matched_lines = matched_lines.rename(columns = {'transcript_text':'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a058fc97-0731-40d0-a629-fae8fab0e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_tmp['line_uid'] = merge_tmp.apply(lambda x :f\"{x.document}_{x.line_number:05d}\", axis=1)\n",
    "textdf['line_uid'] = textdf.apply(lambda x :f\"{x.document}_{x.line_number:05d}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5be9b8b3-fe08-4c54-928f-26f327c9279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = textdf.merge(matched_lines, how='outer', on=['subject', 'folder', 'document', 'line_number', 'text'], indicator=True)\n",
    "assert len(test.query('_merge == \"right_only\"')) == 0\n",
    "merged_text = textdf.merge(matched_lines, how='outer', on=['subject', 'folder', 'document', 'line_number', 'text'], indicator=False)\n",
    "text_with_match = merged_text.query('uid.notnull()')\n",
    "text_no_match = merged_text.query('uid.isnull()')\n",
    "\n",
    "codes = sorted(text_with_match.code.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc849ddf-fd91-456f-8207-871d37b3b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dir = output_dir / 'all'\n",
    "all_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ptnoshort_dir = output_dir / 'pt_noshort'\n",
    "ptnoshort_dir.mkdir(exist_ok=True)\n",
    "\n",
    "turns_dir = output_dir / 'turns'\n",
    "turns_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "226bb9d7-b21c-42bf-be34-64debb7403ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_to_drop = [\n",
    "    # these lines are just parentheticals that can be dropped\n",
    "    'BEH2339 T2 YP JH.docx_00138',\n",
    "    'BEH2339 T2 Parent.docx_00292',\n",
    "    'TAV2131 T2 Therapist.docx_00053',\n",
    "    'BEH2311 T2 Parent.docx_00380',\n",
    "    'TAV2101 T2 Parent.docx_00011',\n",
    "    'TAV2101 T2 Parent.docx_00016',\n",
    "    'BEH2311 T2 Parent.docx_00359',\n",
    "    'ISL2203 T2 Parent.docx_00236',\n",
    "    'TAV2117 T2 Parent.docx_00320',\n",
    "    'TAV2101 Therapist.docx_00021',\n",
    "    'BEH2339 T2 YP JH.docx_00093',\n",
    "    'BEH2339 T2 YP JH.docx_00201',\n",
    "    'ISL2209 Therapist.docx_00123',\n",
    "    'ISL2209 T2 Parent.docx_00076',\n",
    "    'BEH2303 T2 YP.docx_00017',\n",
    "    # These are all a father interjecting in the conversation about a side topic and can be dropped\n",
    "    'BEH2303 T2 YP.docx_00017',\n",
    "    'BEH2303 T2 YP.docx_00070',\n",
    "    'BEH2303 T2 YP.docx_00072',\n",
    "    'BEH2303 T2 YP.docx_00074',\n",
    "    'BEH2303 T2 YP.docx_00076',\n",
    "    'BEH2303 T2 YP.docx_00078',\n",
    "    'BEH2303 T2 YP.docx_00080',\n",
    "    'BEH2303 T2 YP.docx_00082',\n",
    "    'BEH2303 T2 YP.docx_00084',\n",
    "    'BEH2303 T2 YP.docx_00086',\n",
    "    'BEH2303 T2 YP.docx_00088',\n",
    "    'BEH2303 T2 YP.docx_00094',\n",
    "    'BEH2303 T2 YP.docx_00096',\n",
    "    'BEH2303 T2 YP.docx_00228',\n",
    "    'BEH2303 T2 YP.docx_00230',\n",
    "    'BEH2303 T2 YP.docx_00252',\n",
    "    'BEH2303 T2 YP.docx_00255',\n",
    "    'BEH2303 T2 YP.docx_00257',\n",
    "    'BEH2303 T2 YP.docx_00259',\n",
    "    # lines that just say end of interview:\n",
    "    'BEH2311 T2 YP.docx_00198',\n",
    "    'ISL2203 T2 Parent.docx_00239',\n",
    "    'ISL2209 Therapist.docx_00122',\n",
    "    # useless bits that are just easier to drop than to deal with:\n",
    "    'TAV2134 T2 P.docx_00447',\n",
    "    'BEH2336 T2 Parent.docx_00113',\n",
    "    'BEH2332 Therapist.docx_00134',\n",
    "    'TAV2101 Therapist.docx_00018',\n",
    "    'TAV2101 Therapist.docx_00057',\n",
    "    'TAV2145 T2 ES.docx_00003',\n",
    "    'TAV2117 T2 Parent.docx_00205',\n",
    "    'BEH2340 T2 Parent.docx_00138',\n",
    "    'BEH2340 T2 Parent.docx_00140',\n",
    "    'TAV2101 T2 YP.docx_00188',\n",
    "    'TAV2101 T2 YP.docx_00220'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1dbe8c3-0913-49cb-ac11-c8a829af800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = ['a', 'b','c', 'd', 'e', 'f', 'g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c037af78-297a-446b-b376-b01eb053f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:66: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:66: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\('\n",
      "/var/folders/wz/6yqczlkd3fb3rjn025dydkym0000gs/T/ipykernel_45084/67012122.py:66: SyntaxWarning: invalid escape sequence '\\('\n",
      "  to_write.loc[~drop_ind & ~to_write.okprefix & to_write.prefix.str.contains('\\(') & to_write.text.str[6:15].str.contains('P:'), 'prefix'] = 'P:'\n",
      "/var/folders/wz/6yqczlkd3fb3rjn025dydkym0000gs/T/ipykernel_45084/67012122.py:67: SyntaxWarning: invalid escape sequence '\\('\n",
      "  to_write.loc[~drop_ind & ~to_write.okprefix & to_write.prefix.str.contains('\\(') & to_write.text.str[6:15].str.contains('I:'), 'prefix'] = 'I:'\n"
     ]
    }
   ],
   "source": [
    "all_lens = []\n",
    "turn_lens = []\n",
    "ptnoshort_lens = []\n",
    "for code in codes + domains + ['all']:\n",
    "\n",
    "    to_write = merged_text.copy()\n",
    "    to_write['positive'] = False\n",
    "    if code in codes:\n",
    "        to_write.loc[to_write.code == code, 'positive'] = True\n",
    "    elif code in domains:\n",
    "        for cc in codes:\n",
    "            if code in cc:\n",
    "                to_write.loc[to_write.code == code, 'positive'] = True\n",
    "    elif code == 'all':\n",
    "        to_write.loc[to_write.code.notnull(), 'positive'] = True\n",
    "    \n",
    "    to_write = to_write.rename(columns={'folder':'reporter', 'line_uid': 'line_id'}).sort_values(['subject','reporter', 'line_number'])\n",
    "    \n",
    "    drop_ind = (\n",
    "        to_write.text.str.lower().str.contains('age:')\n",
    "        | to_write.text.str.lower().str.contains('gender:')\n",
    "        | to_write.text.str.lower().str.contains('interviewer:')\n",
    "        | to_write.text.str.lower().str.contains('pseudonym:')\n",
    "        | to_write.text.str.lower().str.contains('time point:')\n",
    "        | to_write.line_id.isin(lines_to_drop)\n",
    "    )\n",
    "\n",
    "    okprefixes = ['I:', 'P:', 'P1', 'P2', 'I2', 'P3']\n",
    "    \n",
    "    to_write['prefix'] = to_write.text.str.strip().str[:2].str.strip()\n",
    "    to_write.loc[to_write.text.str.strip().str[:7].str.lower() == 'mother:', 'prefix'] = 'P1'\n",
    "    to_write.loc[to_write.text.str.strip().str[:7].str.lower() == 'father:', 'prefix'] = 'P2'\n",
    "    to_write.loc[to_write.text.str.strip().str[:10].str.lower() == 'therapist:', 'prefix'] = 'I:'\n",
    "    to_write.loc[to_write.text.str.strip().str[:11].str.lower() == 'adolescent:', 'prefix'] = 'P3'\n",
    "    to_write.loc[to_write.text.str.strip().str[:6].str.lower() == 'child:', 'prefix'] = 'P3'\n",
    "    to_write.loc[to_write.text.str.strip().str[:17].str.lower() == 'younger daughter:', 'prefix'] = 'P3'\n",
    "\n",
    "    #actually going to overwrite text so that length filters work\n",
    "    to_write.loc[to_write.text.str.strip().str[:7].str.lower() == 'mother:', 'text'] = 'P1:' + to_write.loc[to_write.text.str.strip().str[:7].str.lower() == 'mother:', 'text'].str[7:]\n",
    "    to_write.loc[to_write.text.str.strip().str[:7].str.lower() == 'father:', 'text'] = 'P2:' + to_write.loc[to_write.text.str.strip().str[:7].str.lower() == 'father:', 'text'].str[7:]\n",
    "    to_write.loc[to_write.text.str.strip().str[:10].str.lower() == 'therapist:', 'text'] = 'I:'  + to_write.loc[to_write.text.str.strip().str[:10].str.lower() == 'therapist:', 'text'].str[10:]\n",
    "    to_write.loc[to_write.text.str.strip().str[:11].str.lower() == 'adolescent:', 'text'] = 'P3:' + to_write.loc[to_write.text.str.strip().str[:11].str.lower() == 'adolescent:', 'text'].str[11:]\n",
    "    to_write.loc[to_write.text.str.strip().str[:6].str.lower() == 'child:', 'text'] = 'P3:' + to_write.loc[to_write.text.str.strip().str[:6].str.lower() == 'child:', 'text'].str[6:]\n",
    "    to_write.loc[to_write.text.str.strip().str[:17].str.lower() == 'younger daughter:', 'text'] = 'P3:' + to_write.loc[to_write.text.str.strip().str[:17].str.lower() == 'younger daughter:', 'text'].str[17:]\n",
    "    \n",
    "    prefix_fixes = [\n",
    "        ('BEH2311 T2 Parent.docx_00379', 'P:'),\n",
    "        ('BEH2314 T2 (P).docx_00065', 'P:'),\n",
    "        ('ISL2203 T2 Parent.docx_00067', 'P:'),\n",
    "        ('ISL2203 T2 Parent.docx_00169', 'P:'),\n",
    "        ('ISL2228 T2 YP.docx_00057', 'P:'),\n",
    "        ('ISL2228 T2 YP.docx_00092', 'P:'),\n",
    "        ('ISL2228 T2 YP.docx_00099', 'P:'),\n",
    "        ('ISL2228 T2 YP.docx_00110', 'P:'),\n",
    "        ('ISL2228 T2 YP.docx_00207', 'P:'),\n",
    "        ('TAV2101 T2 Parent.docx_00027', 'P:'),\n",
    "        ('TAV2131 T2 Therapist.docx_00054', 'P:'),\n",
    "        ('BEH2340 Therapist.docx_00063', 'P:')\n",
    "    ]\n",
    "    for lineid, prefix in prefix_fixes:\n",
    "        to_write.loc[to_write.line_id == lineid, 'prefix'] = prefix\n",
    "        to_write.loc[to_write.line_id == lineid, 'text'] = prefix + ' ' + to_write.loc[to_write.line_id == lineid, 'text'].values[0]\n",
    "    to_write['okprefix'] = to_write.prefix.isin(okprefixes)\n",
    "    \n",
    "    # cleaning prefixes around timestamps\n",
    "    to_write.loc[~drop_ind & ~to_write.okprefix & to_write.prefix.str.contains('\\(') & to_write.text.str[6:15].str.contains('P:'), 'prefix'] = 'P:'\n",
    "    to_write.loc[~drop_ind & ~to_write.okprefix & to_write.prefix.str.contains('\\(') & to_write.text.str[6:15].str.contains('I:'), 'prefix'] = 'I:'\n",
    "    to_write['okprefix'] = to_write.prefix.isin(okprefixes)\n",
    "    \n",
    "    # make sure everything has an ok prefix after fixes\n",
    "    assert len(to_write.loc[~drop_ind & ~to_write.okprefix ]) == 0\n",
    "    # make sure that none of the dropped things are code matches\n",
    "    assert(to_write.loc[drop_ind, 'uid'].notnull().sum() == 0)\n",
    "\n",
    "    to_write['old_reporter'] = to_write.reporter\n",
    "    to_write['reporter'] = to_write.old_reporter.replace({'Adolescent': 'y', 'Parent':'p', 'Therapist':'t'})\n",
    "    to_write['text'] = to_write.text.str.replace('\\n', ' ')\n",
    "    # Use the below to manually check that the first line from each document is meaningfull \n",
    "    #to_write.loc[~drop_ind].groupby('document').first()\n",
    "    to_write = to_write.loc[~drop_ind].copy()\n",
    "    # drop duplicated lines\n",
    "    ## first got to make sure that if any of a line is positive, they all are\n",
    "    to_write['positive'] = to_write.groupby('line_id').positive.transform('any')\n",
    "    to_write = to_write.loc[~to_write.line_id.duplicated()].copy()\n",
    "\n",
    "    # assign turn_ids\n",
    "    turn_ids = []\n",
    "    for document, df in to_write.groupby('document'):\n",
    "        tid = 0\n",
    "        for ix, row in df.reset_index(drop=True).iterrows():\n",
    "            if (ix != 0) and ('I' in row.prefix):\n",
    "                tid += 1\n",
    "            \n",
    "            trow = dict(\n",
    "                line_id=row.line_id,\n",
    "                turn_id=f'{row.document}_{tid:04d}'\n",
    "            )\n",
    "            turn_ids.append(trow)\n",
    "    turn_ids = pd.DataFrame(turn_ids)\n",
    "    \n",
    "    tmp = to_write.merge(turn_ids, how='outer', on='line_id', indicator=True)\n",
    "    assert len(tmp.query(\"_merge != 'both'\")) == 0\n",
    "    to_write = to_write.merge(turn_ids, how='outer', on='line_id')\n",
    "    alltext = to_write.loc[:, ['line_id', 'subject', 'reporter', 'text', 'positive']]\n",
    "    \n",
    "    if alltext.text.str.contains('\\t').sum() > 0:\n",
    "        raise ValueError(\"Text contains a tab, formatting's gonna get broken.\")\n",
    "    if alltext.text.str.contains('\\n').sum() > 0:\n",
    "        raise ValueError(\"Text contains a newline, formatting's gonna get broken.\")\n",
    "    all_filename = all_dir / f'{code}.tsv'\n",
    "    alltext.to_csv(all_filename, sep='\\t', index=None)\n",
    "    all_lens.append(len(alltext))\n",
    "    \n",
    "    turns = to_write.groupby('turn_id')[['subject', 'reporter']].first()\n",
    "    turns['text'] = to_write.groupby('turn_id').text.apply(lambda x:' '.join(x))\n",
    "    turns['positive'] = to_write.groupby('turn_id').positive.any()\n",
    "    turns = turns.reset_index()\n",
    "\n",
    "    if turns.text.str.contains('\\t').sum() > 0:\n",
    "        raise ValueError(\"Text contains a tab, formatting's gonna get broken.\")\n",
    "    if turns.text.str.contains('\\n').sum() > 0:\n",
    "        raise ValueError(\"Text contains a newline, formatting's gonna get broken.\")\n",
    "    turns_filename = turns_dir / f'{code}.tsv'\n",
    "    turns.loc[:, ['turn_id', 'subject', 'reporter', 'text', 'positive']].to_csv(turns_filename, sep='\\t', index=None)\n",
    "    turn_lens.append(len(turns))\n",
    "    \n",
    "    fornoshort = to_write.loc[~to_write.prefix.str.contains('I')]\n",
    "    ptnoshort = fornoshort.groupby('turn_id')[['subject', 'reporter']].first()\n",
    "    ptnoshort['text'] = fornoshort.groupby('turn_id').text.apply(lambda x:' '.join(x))\n",
    "    ptnoshort['positive'] = fornoshort.groupby('turn_id').positive.any()\n",
    "    ptnoshort = ptnoshort.loc[(ptnoshort.text.apply(lambda x: len(x)) > 12)].copy()\n",
    "    # I reviewed the 100 shortest positive utterances longer than 12 characters\n",
    "    # and found some we should drop\n",
    "    uninformative_texts = [\n",
    "    'P: chose one\\u2026',\n",
    "    'P: Yeah, yep.',\n",
    "    'P: and\\u2026 yeah\\u2026',\n",
    "    'P: Yeah true\\u2026',\n",
    "    'P: Yeah, yeah.',\n",
    "    'P: (inaudible)',\n",
    "    'P: Yep, yep...',\n",
    "    \"P: I'd say yes\\u2026\",\n",
    "    'P: For (child)?',\n",
    "    'P: yeah-yeah...',\n",
    "    'P: not angry...',\n",
    "    'P: than she had\\u2026',\n",
    "    'P: And you know,',\n",
    "    'P: Yeah, yeah...',\n",
    "    'P: yeah coz you\\u2026',\n",
    "    'P: 2 years ago...',\n",
    "    'P: No, not really',\n",
    "    'P: Much better...',\n",
    "    'P: Mmm definitely',\n",
    "    'P: Definitely not.',\n",
    "    \"P: I don’t know...\",\n",
    "    'P: Yeah and he was//',\n",
    "    'P: I think kind of...',\n",
    "    'P: sort of regularly\\u2026',\n",
    "    'P: Mm definitely yeah.',\n",
    "    \"P: hmm I'm not sure...\",\n",
    "    'P: hmmm maybe like two\\u2026',\n",
    "    'P: Yes that’s what it is.',\n",
    "    'P: Yes (I: Right) I think so.',\n",
    "    'P: and was able to hold onto\\u2026',\n",
    "    'P: erm... not what did you say...',\n",
    "    'P: Yess. That’s right. Yess, yeah.',\n",
    "    'P: er... erm... I dunno... just I dunno...',\n",
    "]\n",
    "    ptnoshort = ptnoshort.loc[~ptnoshort.text.isin(uninformative_texts)].copy()\n",
    "    ptnoshort = ptnoshort.reset_index()\n",
    "    if ptnoshort.text.str.contains('\\t').sum() > 0:\n",
    "        raise ValueError(\"Text contains a tab, formatting's gonna get broken.\")\n",
    "    if ptnoshort.text.str.contains('\\n').sum() > 0:\n",
    "        raise ValueError(\"Text contains a newline, formatting's gonna get broken.\")\n",
    "    ptnoshort_filename = ptnoshort_dir / f'{code}.tsv'\n",
    "    ptnoshort.loc[:, ['turn_id', 'subject', 'reporter', 'text', 'positive']].to_csv(ptnoshort_filename, sep='\\t', index=None)\n",
    "    ptnoshort_lens.append(len(ptnoshort))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "124f78f0-347b-40d3-b383-ea66da848e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.std(all_lens) == 0\n",
    "assert np.std(turn_lens) == 0\n",
    "assert np.std(ptnoshort_lens) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a78652-c944-4334-abac-b33dfc04c640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
